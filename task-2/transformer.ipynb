{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfff4eed",
   "metadata": {},
   "source": [
    "# Tier C - The Transformer\n",
    "\n",
    "This uses DistilBERT, a lighter transformer model. Transformers use self-attention. This is a mechanism that lets models understand which words matter most in context. For example, in the phrase *\"The girl and her brother\"*, the transformer associates *her* with *The girl* rather than treating each word independently. This context-awareness could help it pick up on the subtle stylistic patterns we identified in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0975f699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/Documents/precog-task/human-or-ai/venv2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "\n",
      "Loading Class 1 (Human-written)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Loading 0: 100%|██████████| 500/500 [00:00<00:00, 8396.77it/s]\n",
      "  Loading 0: 100%|██████████| 500/500 [00:00<00:00, 8313.32it/s]\n",
      "  Loading 0: 100%|██████████| 480/480 [00:00<00:00, 8452.32it/s]\n",
      "  Loading 0: 100%|██████████| 480/480 [00:00<00:00, 8466.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Class 2 (AI-written)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Loading 1: 100%|██████████| 988/988 [00:00<00:00, 7640.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Class 3 (AI-mimicry)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Loading 2: 100%|██████████| 250/250 [00:00<00:00, 8109.89it/s]\n",
      "  Loading 2: 100%|██████████| 250/250 [00:00<00:00, 7766.37it/s]\n",
      "  Loading 2: 100%|██████████| 237/237 [00:00<00:00, 6963.57it/s]\n",
      "  Loading 2: 100%|██████████| 236/236 [00:00<00:00, 7708.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loaded: 3921 total samples\n",
      "  Class 1 (Human): 1960\n",
      "  Class 2 (AI): 988\n",
      "  Class 3 (AI-mimicry): 973\n",
      "\n",
      "Loading Tokenizer for distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Map: 100%|██████████| 3136/3136 [00:00<00:00, 6573.27 examples/s]\n",
      "Map: 100%|██████████| 785/785 [00:00<00:00, 6856.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1012.95it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA (Low-Rank Adaptation)...\n",
      "trainable params: 887,811 || all params: 67,843,590 || trainable%: 1.3086\n",
      "\n",
      "Starting Training (Tier C)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/Documents/precog-task/human-or-ai/venv2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='588' max='588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [588/588 50:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>0.996178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.010092</td>\n",
       "      <td>0.997452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089741</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.998726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/Documents/precog-task/human-or-ai/venv2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "/home/samarth/Documents/precog-task/human-or-ai/venv2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/Documents/precog-task/human-or-ai/venv2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9987\n",
      "Model and tokenizer saved to 'tier_c_final_model'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorWithPadding, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Hello\")\n",
    "\n",
    "MODEL_ID = \"distilbert-base-uncased\"\n",
    "DATASET_DIR = Path('../dataset')\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "def load_texts_from_directory(directory_path, class_label):\n",
    "    data = []\n",
    "    txt_files = glob.glob(os.path.join(str(directory_path), '*.txt'))\n",
    "    \n",
    "    for file_path in tqdm(txt_files, desc=f\"  Loading {class_label}\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "            if text:\n",
    "                data.append({\n",
    "                    'text': text,\n",
    "                    'label': class_label,\n",
    "                    'file_name': os.path.basename(file_path)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"\\nLoading Class 1 (Human-written)...\")\n",
    "class1_data = []\n",
    "for author in ['01-arthur-conan-doyle', '02-pg-wodehouse', '03-mark-twain', '04-william-shakespeare']:\n",
    "    path = DATASET_DIR / 'class1-human-written' / author / 'extracted_paragraphs'\n",
    "    class1_data.extend(load_texts_from_directory(path, 0))\n",
    "\n",
    "print(\"\\nLoading Class 2 (AI-written)...\")\n",
    "class2_path = DATASET_DIR / 'class2-ai-written' / 'ai-generated-paragraphs'\n",
    "class2_data = load_texts_from_directory(class2_path, 1)\n",
    "\n",
    "print(\"\\nLoading Class 3 (AI-mimicry)...\")\n",
    "class3_data = []\n",
    "for author in ['01-arthur-conan-doyle', '02-pg-wodehouse', '03-mark-twain', '04-william-shakespeare']:\n",
    "    path = DATASET_DIR / 'class3-ai-mimicry' / author\n",
    "    class3_data.extend(load_texts_from_directory(path, 2))\n",
    "\n",
    "all_data = class1_data + class2_data + class3_data\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(df)} total samples\")\n",
    "print(f\"  Class 1 (Human): {len(class1_data)}\")\n",
    "print(f\"  Class 2 (AI): {len(class2_data)}\")\n",
    "print(f\"  Class 3 (AI-mimicry): {len(class3_data)}\")\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"test\":  Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "print(f\"\\nLoading Tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Loading Base Model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID, num_labels=3\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA (Low-Rank Adaptation)...\")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False, \n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_checkpoints\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Training (Tier C)...\")\n",
    "trainer.train()\n",
    "print(\"\\nFinal Evaluation on Test Set:\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "model.save_pretrained(\"tier_c_final_model\")\n",
    "tokenizer.save_pretrained(\"tier_c_final_model\")\n",
    "print(\"Model and tokenizer saved to 'tier_c_final_model'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ccfc2",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "It is performing extremely well. Suspiciously well. On first glance, I wonder whether or not it's overfitting...\n",
    "\n",
    "However, I have a hypothesis about this which I will now test. \n",
    "\n",
    "**My Hypothesis:** I think this is genuine, given the vast mathematical and semantic differences in the dataset. Transformers should be able to perform significantly better than the vector space embeddings of the semanticist as well as the pure mathematical state approach of the statistician...\n",
    "\n",
    "Our semanticist was already able to correctly classify with an accuracy of 96%. Now ontop of that we give it the function of attention. [*Attention is all you need,*](https://arxiv.org/pdf/1706.03762) so it would make sense that with attention, the model is able to perform to a significantly higher level of accuracy.\n",
    "\n",
    "I argue that the fact of the matter is that AI is currently neither writes in the same style as any of our selected authors, nor can it accurately mimic the semantic details of any of our authors.\n",
    "\n",
    "**Now, I mentioned that we'd test this.** My plan here is to do a sanity test... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
