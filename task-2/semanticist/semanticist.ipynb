{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6664a859",
   "metadata": {},
   "source": [
    "# Tier B - The Semanticist\n",
    "\n",
    "This section focuses more on the vector embeddings of the text, and their meanings.  \n",
    "\n",
    "**How do we capture that?**  \n",
    "Here, every sentence / piece of text is mapped to a vector high-dimensional vector space. Texts with similar meaning will have similar embeddings in the vector space. For example, the phrase *\"Ferb is smart\"* will be mapped to a vector which is very close to the vector *\"Ferb is intelligent.\"* But *\"Harry is smart\"* will be very far off from the phrase *\"Curse you Perry the platypus!!!\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d569aab",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "We use [Word2Vec](https://arxiv.org/pdf/1301.3781) embeddings with a Multi-Layer Perceptron (MLP). The MLP will classify text based purely on semantic vector embeddings. This is a test to see if our dataset separates texts based on topics or on actual authorship patterns.\n",
    "\n",
    "[Chiny et al., 2023](https://www.researchgate.net/publication/366138831_Effect_of_word_embedding_vector_dimensionality_on_sentiment_analysis_through_short_and_long_texts/citations) is a resource I used to learn about vector embeddings, as well as figure out how many dimensions I need.\n",
    "\n",
    "## Justification for High-Dimensional Embeddings (Tier B)\n",
    "\n",
    "**Decision:**\n",
    "We selected **300-dimensional Word2Vec vector embeddings** by Google (`word2vec-google-news-300`) over the standard 50-dimensional versions for the Semanticist Tier.\n",
    "\n",
    "**Rationale:**\n",
    "1. The standard 50-dimensional embeddings are trained on a corpus of 6 billion tokens (Wikipedia 2014). In contrast, the 300-dimensional embeddings are trained on 840 billion tokens from the [Common Crawl](https://commoncrawl.org/) dataset. This represents a 140x increase in the model's exposure to linguistic contexts, allowing it to capture semantic distinctions (e.g., the difference between \"melancholy\" and \"sad\").\n",
    "2. Tier B utilizes a \"Bag of Means\" approach [2], where individual word vectors are averaged to create a single paragraph vector. This averaging process is inherently destructive to information.\n",
    "    * Averaging low-dimensional vectors (50d) results in a bad representation where distinct authors become indistinguishable.\n",
    "    * High-dimensional vectors (300d) provide a larger mathematical \"volume,\" ensuring that the averaged vector retains sufficient unique signal to act as a classifier between the human authors and AI.\n",
    "\n",
    "## NOTE on why Word2Vec instead of GloVe** \n",
    "- We chose Word2Vec over GloVe for its Skip-gram and CBOW architecture, which is better for larger vocabularies. [source](https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec)\n",
    "\n",
    "An Explanation:\n",
    "\n",
    "Word2Vec has a pretty simple architecture. It's just a 2-layer NN, it deals with addition of new words in the vocabulary, and it preserves the relationoship between words. Training Word2Vec is also quite resource intensive, as it requires a large amount of RAM to store the vocabulary of the corpus.\n",
    "\n",
    "CBOW (Continuous Bag of Words) - predicts the target word from the context.\n",
    "SkipGram - predicts context words from the target word.\n",
    "\n",
    "**NOTE:** I also remove `stopwords` like 'and', 'in', 'the' etc because they are just noise in the embedding as mentioned by [2]\n",
    "\n",
    "**Reference:**\n",
    "1. [*Pennington et al., 2014. GloVe: Global Vectors for Word Representation.*](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "2. [*Effect of word embedding vector dimensionality on sentiment analysis through short and long texts (International Journal of Social Sciences Bulletin).*](https://www.researchgate.net/publication/366138831_Effect_of_word_embedding_vector_dimensionality_on_sentiment_analysis_through_short_and_long_texts/citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d2a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec-google-news-300... (This may take a few minutes on first run)\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Embeddings loaded successfully.\n",
      "\n",
      "Loading Class 1 (Human-written)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Loading 0: 100%|██████████| 500/500 [00:00<00:00, 8332.25it/s]\n",
      "  Loading 0: 100%|██████████| 500/500 [00:00<00:00, 5328.64it/s]\n",
      "  Loading 0: 100%|██████████| 480/480 [00:00<00:00, 8313.51it/s]\n",
      "  Loading 0: 100%|██████████| 480/480 [00:00<00:00, 8530.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Class 2 (AI-written)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Loading 1: 100%|██████████| 988/988 [00:00<00:00, 6344.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Class 3 (AI-mimicry)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Loading 2: 100%|██████████| 250/250 [00:00<00:00, 7902.51it/s]\n",
      "  Loading 2: 100%|██████████| 250/250 [00:00<00:00, 7493.90it/s]\n",
      "  Loading 2: 100%|██████████| 237/237 [00:00<00:00, 6295.56it/s]\n",
      "  Loading 2: 100%|██████████| 236/236 [00:00<00:00, 7482.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loaded: 3921 total samples\n",
      "  Class 1 (Human): 1960\n",
      "  Class 2 (AI): 988\n",
      "  Class 3 (AI-mimicry): 973\n",
      "\n",
      "Vectorizing paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████| 3921/3921 [00:00<00:00, 6839.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BINARY CLASSIFICATION RESULTS - SEMANTICIST\n",
      "\n",
      "Class 1 (Human) vs Class 2 (AI): 0.9746 (97.46%)\n",
      "Confusion Matrix:\n",
      "[[388   4]\n",
      " [ 11 187]]\n",
      "\n",
      "Class 1 (Human) vs Class 3 (AI-mimicry): 0.9591 (95.91%)\n",
      "Confusion Matrix:\n",
      "[[377  15]\n",
      " [  9 186]]\n",
      "\n",
      "Class 2 (AI) vs Class 3 (AI-mimicry): 0.9695 (96.95%)\n",
      "Confusion Matrix:\n",
      "[[190   8]\n",
      " [  4 191]]\n",
      "\n",
      "Multi-class (All 3 classes): 0.9618 (96.18%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMBEDDING_MODEL_NAME = 'word2vec-google-news-300' \n",
    "DATASET_DIR = Path('../../dataset')\n",
    "\n",
    "STOPWORDS = set([\n",
    "    'the', 'and', 'is', 'in', 'to', 'of', 'a', 'an', 'that', 'it', 'for', 'on', \n",
    "    'with', 'as', 'was', 'at', 'by', 'be', 'this', 'which', 'or', 'from'\n",
    "])\n",
    "\n",
    "print(f\"Loading {EMBEDDING_MODEL_NAME}... (This may take a few minutes on first run)\")\n",
    "wv = api.load(EMBEDDING_MODEL_NAME)\n",
    "print(\"Embeddings loaded successfully.\")\n",
    "\n",
    "def text_to_average_vector(text):\n",
    "    \"\"\"\n",
    "    Converts a paragraph into a single 300-dimensional vector\n",
    "    by averaging the vectors of its meaningful words.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(300) \n",
    "    words = text.lower().split()\n",
    "    \n",
    "    valid_vectors = [\n",
    "        wv[word] for word in words \n",
    "        if word in wv and word not in STOPWORDS\n",
    "    ]\n",
    "    \n",
    "    if len(valid_vectors) == 0:\n",
    "        return np.zeros(300)\n",
    "    \n",
    "    return np.mean(valid_vectors, axis=0)\n",
    "\n",
    "def load_texts_from_directory(directory_path, class_label):\n",
    "    \"\"\"Load all text files from a directory\"\"\"\n",
    "    data = []\n",
    "    txt_files = glob.glob(os.path.join(str(directory_path), '*.txt'))\n",
    "    \n",
    "    for file_path in tqdm(txt_files, desc=f\"  Loading {class_label}\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "            if text:\n",
    "                data.append({\n",
    "                    'text': text,\n",
    "                    'label': class_label,\n",
    "                    'file_name': os.path.basename(file_path)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load Class 1 (Human-written)\n",
    "print(\"\\nLoading Class 1 (Human-written)...\")\n",
    "class1_data = []\n",
    "for author in ['01-arthur-conan-doyle', '02-pg-wodehouse', '03-mark-twain', '04-william-shakespeare']:\n",
    "    path = DATASET_DIR / 'class1-human-written' / author / 'extracted_paragraphs'\n",
    "    class1_data.extend(load_texts_from_directory(path, 0))\n",
    "\n",
    "# Load Class 2 (AI-written)\n",
    "print(\"\\nLoading Class 2 (AI-written)...\")\n",
    "class2_path = DATASET_DIR / 'class2-ai-written' / 'ai-generated-paragraphs'\n",
    "class2_data = load_texts_from_directory(class2_path, 1)\n",
    "\n",
    "# Load Class 3 (AI-mimicry)\n",
    "print(\"\\nLoading Class 3 (AI-mimicry)...\")\n",
    "class3_data = []\n",
    "for author in ['01-arthur-conan-doyle', '02-pg-wodehouse', '03-mark-twain', '04-william-shakespeare']:\n",
    "    path = DATASET_DIR / 'class3-ai-mimicry' / author\n",
    "    class3_data.extend(load_texts_from_directory(path, 2))\n",
    "\n",
    "# Combine all data\n",
    "all_data = class1_data + class2_data + class3_data\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(df)} total samples\")\n",
    "print(f\"  Class 1 (Human): {len(class1_data)}\")\n",
    "print(f\"  Class 2 (AI): {len(class2_data)}\")\n",
    "print(f\"  Class 3 (AI-mimicry): {len(class3_data)}\")\n",
    "\n",
    "print(\"\\nVectorizing paragraphs\")\n",
    "X = np.array([text_to_average_vector(text) for text in tqdm(df['text'], desc=\"Vectorizing\")])\n",
    "y = df['label'].values\n",
    "\n",
    "print(\"\\nBINARY CLASSIFICATION RESULTS - SEMANTICIST\")\n",
    "\n",
    "# Binary Classification 1: Class 1 vs Class 2\n",
    "mask_12 = (y == 0) | (y == 1)\n",
    "X_12, y_12 = X[mask_12], y[mask_12]\n",
    "df_12 = df[mask_12].reset_index(drop=True)\n",
    "\n",
    "X_train_12, X_test_12, y_train_12, y_test_12, idx_train_12, idx_test_12 = train_test_split(\n",
    "    X_12, y_12, df_12.index, test_size=0.2, stratify=y_12, random_state=42\n",
    ")\n",
    "\n",
    "clf_12 = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64), \n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,      \n",
    "    random_state=42,    \n",
    "    early_stopping=True \n",
    ")\n",
    "clf_12.fit(X_train_12, y_train_12)\n",
    "y_pred_12 = clf_12.predict(X_test_12)\n",
    "accuracy_12 = accuracy_score(y_test_12, y_pred_12)\n",
    "print(f\"\\nClass 1 (Human) vs Class 2 (AI): {accuracy_12:.4f} ({accuracy_12*100:.2f}%)\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_12, y_pred_12))\n",
    "\n",
    "# Binary Classification 2: Class 1 vs Class 3\n",
    "mask_13 = (y == 0) | (y == 2)\n",
    "X_13, y_13 = X[mask_13], y[mask_13]\n",
    "df_13 = df[mask_13].reset_index(drop=True)\n",
    "# Remap: 0 stays 0, 2 becomes 1\n",
    "y_13_binary = np.where(y_13 == 2, 1, y_13)\n",
    "\n",
    "X_train_13, X_test_13, y_train_13, y_test_13, idx_train_13, idx_test_13 = train_test_split(\n",
    "    X_13, y_13_binary, df_13.index, test_size=0.2, stratify=y_13_binary, random_state=42\n",
    ")\n",
    "\n",
    "clf_13 = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64), \n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,      \n",
    "    random_state=42,    \n",
    "    early_stopping=True \n",
    ")\n",
    "clf_13.fit(X_train_13, y_train_13)\n",
    "y_pred_13 = clf_13.predict(X_test_13)\n",
    "accuracy_13 = accuracy_score(y_test_13, y_pred_13)\n",
    "print(f\"\\nClass 1 (Human) vs Class 3 (AI-mimicry): {accuracy_13:.4f} ({accuracy_13*100:.2f}%)\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_13, y_pred_13))\n",
    "\n",
    "# Binary Classification 3: Class 2 vs Class 3\n",
    "mask_23 = (y == 1) | (y == 2)\n",
    "X_23, y_23 = X[mask_23], y[mask_23]\n",
    "df_23 = df[mask_23].reset_index(drop=True)\n",
    "# Remap: 1 becomes 0, 2 becomes 1\n",
    "y_23_binary = np.where(y_23 == 1, 0, 1)\n",
    "\n",
    "X_train_23, X_test_23, y_train_23, y_test_23, idx_train_23, idx_test_23 = train_test_split(\n",
    "    X_23, y_23_binary, df_23.index, test_size=0.2, stratify=y_23_binary, random_state=42\n",
    ")\n",
    "\n",
    "clf_23 = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64), \n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,      \n",
    "    random_state=42,    \n",
    "    early_stopping=True \n",
    ")\n",
    "clf_23.fit(X_train_23, y_train_23)\n",
    "y_pred_23 = clf_23.predict(X_test_23)\n",
    "accuracy_23 = accuracy_score(y_test_23, y_pred_23)\n",
    "print(f\"\\nClass 2 (AI) vs Class 3 (AI-mimicry): {accuracy_23:.4f} ({accuracy_23*100:.2f}%)\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_23, y_pred_23))\n",
    "\n",
    "# Keep the full model for later analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64), \n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,      \n",
    "    random_state=42,    \n",
    "    early_stopping=True \n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nMulti-class (All 3 classes): {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2a916",
   "metadata": {},
   "source": [
    "## Misclassified Texts\n",
    "\n",
    "Now let's save all misclassified examples to understand where the Semanticist model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d19de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1 files to class1_as_class2.txt\n",
      "Saved 9 files to class1_as_class3.txt\n",
      "Saved 6 files to class2_as_class1.txt\n",
      "Saved 11 files to class3_as_class1.txt\n",
      "Saved 3 files to class3_as_class2.txt\n",
      "\n",
      "Total: 30 misclassified text files saved to semanticist_misclassified/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'semanticist_misclassified'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# We need to map back to the full dataset to get test indices\n",
    "test_indices = y_test.copy()\n",
    "\n",
    "# Create results dataframe with file names\n",
    "# Get the original indices from the train_test_split\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "_, test_idx = tts(range(len(df)), test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred,\n",
    "    'file_name': df.iloc[test_idx]['file_name'].values\n",
    "})\n",
    "\n",
    "reverse_mapping = {0: 'Class 1: Human-written', 1: 'Class 2: AI-written', 2: 'Class 3: AI-mimicry'}\n",
    "\n",
    "# Define misclassification categories\n",
    "categories = [\n",
    "    (0, 1, 'class1_as_class2.txt', 'Class 1 (Human) misclassified as Class 2 (AI)'),\n",
    "    (0, 2, 'class1_as_class3.txt', 'Class 1 (Human) misclassified as Class 3 (AI-mimicry)'),\n",
    "    (1, 0, 'class2_as_class1.txt', 'Class 2 (AI) misclassified as Class 1 (Human)'),\n",
    "    (1, 2, 'class2_as_class3.txt', 'Class 2 (AI) misclassified as Class 3 (AI-mimicry)'),\n",
    "    (2, 0, 'class3_as_class1.txt', 'Class 3 (AI-mimicry) misclassified as Class 1 (Human)'),\n",
    "    (2, 1, 'class3_as_class2.txt', 'Class 3 (AI-mimicry) misclassified as Class 2 (AI)')\n",
    "]\n",
    "\n",
    "total_saved = 0\n",
    "\n",
    "for actual_class, predicted_class, filename, description in categories:\n",
    "    # Filter misclassified examples for this category\n",
    "    category_misclassified = results_df[(results_df['actual'] == actual_class) & \n",
    "                                        (results_df['predicted'] == predicted_class)]\n",
    "    \n",
    "    if len(category_misclassified) == 0:\n",
    "        continue\n",
    "    \n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(\"-\" * 80 + \"\\n\")\n",
    "        outfile.write(f\"{description}\\n\")\n",
    "        outfile.write(f\"Total: {len(category_misclassified)} files\\n\")\n",
    "        outfile.write(\"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for idx, row in category_misclassified.iterrows():\n",
    "            text_file = row['file_name']\n",
    "            actual = row['actual']\n",
    "            predicted = row['predicted']\n",
    "            \n",
    "            # Construct the full path to the text file\n",
    "            if actual == 0:\n",
    "                actual_class_folder = 'class1-human-written'\n",
    "            elif actual == 1:\n",
    "                actual_class_folder = 'class2-ai-written'\n",
    "            else:\n",
    "                actual_class_folder = 'class3-ai-mimicry'\n",
    "            \n",
    "            # Find the file in the dataset\n",
    "            base_path = '../../dataset'\n",
    "            file_path = None\n",
    "            \n",
    "            # Search for the file\n",
    "            for root, dirs, files in os.walk(os.path.join(base_path, actual_class_folder)):\n",
    "                if text_file in files:\n",
    "                    file_path = os.path.join(root, text_file)\n",
    "                    break\n",
    "            \n",
    "            if file_path and os.path.exists(file_path):\n",
    "                outfile.write(\"-\" * 80 + \"\\n\")\n",
    "                outfile.write(f\"File: {text_file}\\n\")\n",
    "                outfile.write(f\"Actual: {reverse_mapping[actual]}\\n\")\n",
    "                outfile.write(f\"Predicted: {reverse_mapping[predicted]}\\n\")\n",
    "                outfile.write(\"-\" * 80 + \"\\n\")\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        outfile.write(content)\n",
    "                except Exception as e:\n",
    "                    outfile.write(f\"Error reading file: {e}\\n\")\n",
    "                \n",
    "                outfile.write(\"\\n\\n\")\n",
    "            else:\n",
    "                outfile.write(f\"Could not find file: {text_file}\\n\\n\")\n",
    "    \n",
    "    total_saved += len(category_misclassified)\n",
    "    print(f\"Saved {len(category_misclassified)} files to {filename}\")\n",
    "\n",
    "print(f\"\\nTotal: {total_saved} misclassified text files saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859965a7",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Class 1 vs Class 2 reports 97.46% accuracy.\n",
    "Class 1 vs Class 3 reports 95.91% accuracy.\n",
    "\n",
    "This is amazing accuracy... The semanticist is far outperforming the statistician, and it is clear that Word2Vec is able to clearly depict the nuance of human vs AI texts.\n",
    "\n",
    "Now, I'll try to analyse how / why I got these findings.\n",
    "\n",
    "\n",
    "## What We Found\n",
    "The semanticist significantly outperformed the statistician. Even though the AI can count commas and mimic sentence lengths, it fails to fake the semantics which humans are able to uniquely author.\n",
    "\n",
    "Detecting class-2 was v easy. The model saw that the AI stuck to very safe, average word choices (low variance). The Human author used concrete, specific nouns, while the Generic AI used broader, more predictable language. \n",
    "\n",
    "It detected class-3 quite easily as well, showing that despite good prompt engineering, the semantic structures are still quite different.\n",
    "\n",
    "# Visualising the vector space\n",
    "\n",
    "I now want to see how the classes are visualised in the multi-dimensional vector space. Since it is impossible for us to envision a 4th dimension, let alone the 300th, I will use a technique called dimensionality reduction. Dimensionality reduction uses PCA to preserve the overall structure, while still keeping vectors close to vectors that they should be close to in the new 3D vector space. This can be done using using scikitlearn's PCA library. We also use it's t-SNE library, which is good for preserving local clusters and seeing neighbourss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db9deb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing 300 dimensions to 3 dimensions using PCA...\n",
      "Original shape: (785, 300) (785 paragraphs × 300 dimensions)\n",
      "Reduced shape: (785, 3) (785 paragraphs × 3 dimensions)\n",
      "Variance explained by 3 components: 28.82%\n",
      "\n",
      "Ready to visualize! The 300D semantic space has been compressed to 3D.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# This is where we use the 300-dimensional vector embeddings from earlier!\n",
    "# X_test contains the averaged Word2Vec embeddings for each test paragraph\n",
    "print(\"Reducing 300 dimensions to 3 dimensions using PCA...\")\n",
    "print(f\"Original shape: {X_test.shape} (785 paragraphs × 300 dimensions)\")\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_3d = pca.fit_transform(X_test)\n",
    "\n",
    "print(f\"Reduced shape: {X_3d.shape} (785 paragraphs × 3 dimensions)\")\n",
    "print(f\"Variance explained by 3 components: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Create dataframe for visualization\n",
    "viz_df = pd.DataFrame(data=X_3d, columns=['PC1', 'PC2', 'PC3'])\n",
    "viz_df['Label'] = y_test  # The actual class labels (0, 1, or 2)\n",
    "label_map = {0: 'Human', 1: 'Generic AI', 2: 'Mimic AI'}\n",
    "viz_df['Class'] = viz_df['Label'].map(label_map)\n",
    "\n",
    "print(\"\\nReady to visualize! The 300D semantic space has been compressed to 3D.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2121a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classes visualization saved\n",
      "\n",
      "All visualizations saved to semanticist-visuals/ directory!\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "os.makedirs('semanticist-visuals', exist_ok=True)\n",
    "\n",
    "color_map = {\n",
    "    'Human': '#1f77b4',\n",
    "    'Generic AI': '#ff7f0e',\n",
    "    'Mimic AI': '#2ca02c'\n",
    "}\n",
    "\n",
    "fig_all = px.scatter_3d(\n",
    "    viz_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    z='PC3',\n",
    "    color='Class',\n",
    "    color_discrete_map=color_map,\n",
    "    title='All 3 Classes - Interactive 3D Semantic Space',\n",
    "    labels={'PC1': 'PC 1', 'PC2': 'PC 2', 'PC3': 'PC 3'},\n",
    "    opacity=0.6,\n",
    "    height=800\n",
    ")\n",
    "fig_all.update_traces(marker=dict(size=4))\n",
    "fig_all.update_layout(\n",
    "    scene=dict(xaxis_title='PC 1', yaxis_title='PC 2', zaxis_title='PC 3'),\n",
    "    font=dict(size=12)\n",
    ")\n",
    "fig_all.write_html('semanticist-visuals/all_classes.html')\n",
    "print(\"All classes visualization saved\")\n",
    "\n",
    "print(\"\\nAll visualizations saved to semanticist-visuals/ directory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a2246",
   "metadata": {},
   "source": [
    "~ visualisation codes generated by Claude Sonnet 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681ee8b",
   "metadata": {},
   "source": [
    "If anything, this just shows us why 3-dimensions would not have been good enough imo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
