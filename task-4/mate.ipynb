{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91f6515",
   "metadata": {},
   "source": [
    "# Memetic Algorithm for Text Evolution (MATE)\n",
    "\n",
    "This is where I code and try out MATE. MATE has been explained significantly in [README.md](README.md). Go through that to know how I plan to use a memetic algorithm to jailbreak our tier-c model :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2d7e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import shap\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Setup directories\n",
    "POPULATION_DIR = Path(\"population\")\n",
    "EVOLUTION_DIR = Path(\"evolution\")\n",
    "INPUT_FILE = Path(\"input.txt\")\n",
    "MODEL_PATH = Path(\"../task-2/transformer/tier_c_final_model\")\n",
    "\n",
    "print(\"Directories configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7051ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading classifier model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1196.62it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n",
      "  Base: distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Classifier Model\n",
    "print(\"Loading classifier model...\")\n",
    "\n",
    "# Load PEFT configuration\n",
    "peft_model_id = str(MODEL_PATH)\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,  # \"distilbert-base-uncased\"\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "model.eval()\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"  Base: {config.base_model_name_or_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad6a5955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Gemini API...\n",
      "Gemini API initialized\n",
      "Model: gemini-pro-latest\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize Gemini API\n",
    "print(\"Initializing Gemini API...\")\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file!\")\n",
    "\n",
    "# Initialize client\n",
    "client = genai.Client(api_key=api_key)\n",
    "MODEL_NAME = \"gemini-pro-latest\" \n",
    "\n",
    "print(f\"Gemini API initialized\")\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f9e9ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gemini API with CORRECTED max_output_tokens...\n",
      "Response received successfully!\n",
      "\n",
      "Finish reason: FinishReason.STOP\n",
      "\n",
      "Generated text (695 chars):\n",
      "Technology has become an inseparable extension of modern human existence, a powerful force that reshapes how we live, work, and connect. At its best, it acts as a great equalizer, democratizing access to information and bridging geographical divides with instantaneous communication. Yet, this same connectivity introduces complex challenges, from navigating issues of privacy and digital well-being to grappling with the rapid pace of change that can leave some behind. Ultimately, technology is a double-edged sword, a tool whose ultimate impact—whether it builds a more inclusive future or deepens existing divisions—is continually shaped by the choices we make in how we create and wield it.\n",
      "\n",
      "✓ API is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.5: [ DEBUG ] Test Gemini API - FIXED\n",
    "print(\"Testing Gemini API with CORRECTED max_output_tokens...\")\n",
    "\n",
    "test_prompt = \"Write a short paragraph about technology.\"\n",
    "\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=test_prompt,\n",
    "        config={\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"max_output_tokens\": 8192,  # FIXED: Was 512, causing MAX_TOKENS error\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Response received successfully!\")\n",
    "    \n",
    "    if response.candidates:\n",
    "        candidate = response.candidates[0]\n",
    "        print(f\"\\nFinish reason: {candidate.finish_reason}\")\n",
    "        \n",
    "        if candidate.content and candidate.content.parts:\n",
    "            raw_text = candidate.content.parts[0].text\n",
    "            print(f\"\\nGenerated text ({len(raw_text)} chars):\")\n",
    "            print(raw_text)\n",
    "            print(\"\\n✓ API is working correctly!\")\n",
    "        else:\n",
    "            print(\"ERROR: Still no content/parts\")\n",
    "    else:\n",
    "        print(\"ERROR: No candidates in response\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed7d8be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer for semantic similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1254.30it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Semantic model loaded\n",
      "  Model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Semantic Similarity Model\n",
    "print(\"Loading sentence transformer for semantic similarity...\")\n",
    "\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"✓ Semantic model loaded\")\n",
    "print(f\"  Model: all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a34b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction:\n",
      "  Human: 0.00253329\n",
      "  AI-written: 0.99505049\n",
      "  AI-mimicry: 0.00241629\n",
      "  log(P(Human)): -5.97823811\n",
      "  Predicted: Class 2 (conf=0.9951)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Helper Functions - Classifier Prediction\n",
    "def get_classifier_predictions(text):\n",
    "    \"\"\"\n",
    "    Get classifier predictions for a text.\n",
    "    Returns: (probs, predicted_class, confidence)\n",
    "    \n",
    "    IMPORTANT: Internal model uses 0-indexed classes (0, 1, 2)\n",
    "    But project uses 1-indexed labels (Class 1, 2, 3):\n",
    "      - Class 1 (Human)       → probs[0], pred_class=0\n",
    "      - Class 2 (AI-written)  → probs[1], pred_class=1  \n",
    "      - Class 3 (AI-mimicry)  → probs[2], pred_class=2\n",
    "    \n",
    "    When displaying, add +1: \"Class {pred_class+1}\"\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    predicted_class = np.argmax(probs)\n",
    "    confidence = probs[predicted_class]\n",
    "    \n",
    "    return probs, predicted_class, confidence\n",
    "\n",
    "def get_human_probability(text):\n",
    "    \"\"\"Get P(Human) = P(Class 1) for a text.\"\"\"\n",
    "    probs, _, _ = get_classifier_predictions(text)\n",
    "    return probs[0]\n",
    "\n",
    "def get_log_human_probability(text):\n",
    "    \"\"\"Get log(P(Human)) for better numerical stability when P(Human) is very small.\"\"\"\n",
    "    probs, _, _ = get_classifier_predictions(text)\n",
    "    return np.log(probs[0] + 1e-10)  # Add epsilon to avoid log(0)\n",
    "\n",
    "# Test on a simple example\n",
    "test_text = \"Technology has become an integral part of modern life.\"\n",
    "probs, pred_class, conf = get_classifier_predictions(test_text)\n",
    "print(f\"Test prediction:\")\n",
    "print(f\"  P(Human):      {probs[0]:.8f}  [Class 1]\")\n",
    "print(f\"  P(AI-written): {probs[1]:.8f}  [Class 2]\")\n",
    "print(f\"  P(AI-mimicry): {probs[2]:.8f}  [Class 3]\")\n",
    "print(f\"  log(P(Human)): {np.log(probs[0] + 1e-10):.8f}\")\n",
    "print(f\"  Predicted: Class {pred_class+1} (internal idx={pred_class}, conf={conf:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2718af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity test:\n",
      "  Similar texts: 0.5508\n",
      "  Different texts: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: [DEBUG] Helper Functions - Semantic Similarity\n",
    "def get_semantic_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two texts using sentence embeddings.\n",
    "    Returns: similarity score in [0, 1]\n",
    "    \"\"\"\n",
    "    embeddings = semantic_model.encode([text1, text2])\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Test\n",
    "text1 = \"The cat sat on the mat.\"\n",
    "text2 = \"A feline rested upon the rug.\"\n",
    "text3 = \"The weather is nice today.\"\n",
    "\n",
    "sim_similar = get_semantic_similarity(text1, text2)\n",
    "sim_different = get_semantic_similarity(text1, text3)\n",
    "\n",
    "print(f\"Semantic similarity test:\")\n",
    "print(f\"  Similar texts: {sim_similar:.4f}\")\n",
    "print(f\"  Different texts: {sim_different:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56bf1682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity test:\n",
      "  Fluent text: 1.0804\n",
      "  Less fluent: 1.5800\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Helper Functions - Perplexity Estimation\n",
    "def estimate_perplexity(text):\n",
    "    \"\"\"\n",
    "    Estimate perplexity using the classifier model as a proxy.\n",
    "    Lower values = more fluent text.\n",
    "    Note: This is a rough approximation since we don't have a dedicated language model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use negative log likelihood as proxy\n",
    "        # Higher confidence = lower perplexity\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        max_prob = torch.max(probs).item()\n",
    "        \n",
    "        # Rough perplexity estimate: inverse of confidence\n",
    "        # This is not true perplexity but serves as fluency indicator\n",
    "        perplexity = 1.0 / (max_prob + 1e-10)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Test\n",
    "fluent_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "less_fluent = \"The quick brown fox the lazy dog over jumps.\"\n",
    "\n",
    "ppl_fluent = estimate_perplexity(fluent_text)\n",
    "ppl_less = estimate_perplexity(less_fluent)\n",
    "\n",
    "print(f\"Perplexity test:\")\n",
    "print(f\"  Fluent text: {ppl_fluent:.4f}\")\n",
    "print(f\"  Less fluent: {ppl_less:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ba84a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness test (HIGH PRECISION):\n",
      "  Fitness: -4.76383018\n",
      "  P(Human): 0.00941421\n",
      "  log(P(Human)): -4.66553497\n",
      "  Similarity: 0.8009\n",
      "  Perplexity: 1.0135\n",
      "  Semantic penalty: 0.09829509\n",
      "  Perplexity penalty: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Fitness Function with Lagrangian Relaxation\n",
    "def calculate_fitness(text, original_text, lambda_semantic=2.0, lambda_perplexity=0.5, \n",
    "                     semantic_threshold=0.85, perplexity_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Calculate fitness score with Lagrangian relaxation.\n",
    "    \n",
    "    Objective: Maximize P(Human) = Minimize P(AI)\n",
    "    Constraints (soft penalties):\n",
    "    - Semantic drift beyond threshold\n",
    "    - Excessive perplexity\n",
    "    \n",
    "    Args:\n",
    "        text: Candidate text\n",
    "        original_text: Original text for semantic comparison\n",
    "        lambda_semantic: Penalty weight for semantic drift\n",
    "        lambda_perplexity: Penalty weight for perplexity\n",
    "        semantic_threshold: Minimum acceptable similarity (0.85-0.90)\n",
    "        perplexity_threshold: Maximum acceptable perplexity\n",
    "    \n",
    "    Returns:\n",
    "        fitness: Higher is better\n",
    "        components: Dict with breakdown\n",
    "    \"\"\"\n",
    "    # Primary objective: maximize P(Human)\n",
    "    # Use log probabilities to avoid saturation at 0.0000\n",
    "    p_human = get_human_probability(text)\n",
    "    log_p_human = get_log_human_probability(text)\n",
    "    \n",
    "    # Constraint 1: Semantic similarity\n",
    "    similarity = get_semantic_similarity(text, original_text)\n",
    "    semantic_penalty = 0.0\n",
    "    if similarity < semantic_threshold:\n",
    "        semantic_penalty = lambda_semantic * (semantic_threshold - similarity)\n",
    "    \n",
    "    # Constraint 2: Perplexity\n",
    "    perplexity = estimate_perplexity(text)\n",
    "    perplexity_penalty = 0.0\n",
    "    if perplexity > perplexity_threshold:\n",
    "        perplexity_penalty = lambda_perplexity * (perplexity - perplexity_threshold)\n",
    "    \n",
    "    # Final fitness (higher is better)\n",
    "    # Use log space to preserve gradient when P(Human) is tiny\n",
    "    fitness = log_p_human - semantic_penalty - perplexity_penalty\n",
    "    \n",
    "    components = {\n",
    "        'fitness': fitness,\n",
    "        'p_human': p_human,\n",
    "        'log_p_human': log_p_human,\n",
    "        'similarity': similarity,\n",
    "        'perplexity': perplexity,\n",
    "        'semantic_penalty': semantic_penalty,\n",
    "        'perplexity_penalty': perplexity_penalty\n",
    "    }\n",
    "    \n",
    "    return fitness, components\n",
    "\n",
    "# Test\n",
    "original = \"Technology has become an integral part of modern life.\"\n",
    "candidate = \"Tech has become a key part of everyday life.\"\n",
    "\n",
    "fitness, comp = calculate_fitness(candidate, original)\n",
    "print(f\"Fitness test (HIGH PRECISION):\")\n",
    "print(f\"  Fitness: {fitness:.8f}\")\n",
    "print(f\"  P(Human): {comp['p_human']:.8f}\")\n",
    "print(f\"  log(P(Human)): {comp['log_p_human']:.8f}\")\n",
    "print(f\"  Similarity: {comp['similarity']:.4f}\")\n",
    "print(f\"  Perplexity: {comp['perplexity']:.4f}\")\n",
    "print(f\"  Semantic penalty: {comp['semantic_penalty']:.8f}\")\n",
    "print(f\"  Perplexity penalty: {comp['perplexity_penalty']:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76845ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token saliency test:\n",
      "  Total tokens: 17\n",
      "  Top 3 mutable tokens:\n",
      "    [1] technology: 24.1031\n",
      "    [8] modern: 22.2101\n",
      "    [5] integral: 17.7286\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: SHAP-based Token Saliency\n",
    "def get_token_saliency(text, target_class=1):\n",
    "    \"\"\"\n",
    "    Compute token-level saliency using gradient-based approach.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        target_class: Which class to compute saliency for (1=Human, 2=AI-written, 3=AI-mimicry)\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of tokens\n",
    "        saliency_scores: Importance score for each token\n",
    "        top_k_indices: Indices of most important tokens\n",
    "    \"\"\"\n",
    "    model.train()  # Enable gradients\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Get embeddings and enable gradients\n",
    "    embeddings = model.get_base_model().distilbert.embeddings.word_embeddings(input_ids)\n",
    "    embeddings.requires_grad = True\n",
    "    \n",
    "    # Forward pass with custom embeddings\n",
    "    outputs = model.get_base_model().distilbert(\n",
    "        inputs_embeds=embeddings,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    \n",
    "    # Get logits\n",
    "    pooled_output = outputs[0][:, 0]  # CLS token\n",
    "    logits = model.get_base_model().classifier(pooled_output)\n",
    "    \n",
    "    # Focus on target class\n",
    "    target_logit = logits[0, target_class - 1]  # Adjust for 0-indexing\n",
    "    \n",
    "    # Backward pass\n",
    "    target_logit.backward()\n",
    "    \n",
    "    # Get gradients\n",
    "    saliency = embeddings.grad.abs().sum(dim=-1).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Convert to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Get top-k indices (excluding special tokens)\n",
    "    valid_indices = [i for i, tok in enumerate(tokens) \n",
    "                    if tok not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    \n",
    "    # Sort by saliency\n",
    "    token_saliency = [(i, tokens[i], saliency[i]) for i in valid_indices]\n",
    "    token_saliency.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Get top 20% indices\n",
    "    top_k = max(3, len(valid_indices) // 5)\n",
    "    top_k_indices = [t[0] for t in token_saliency[:top_k]]\n",
    "    \n",
    "    model.eval()  # Disable gradients\n",
    "    \n",
    "    return tokens, saliency, top_k_indices, token_saliency\n",
    "\n",
    "# Test\n",
    "test_text = \"Technology has become an integral part of modern life, shaping how we work.\"\n",
    "tokens, saliency, top_indices, token_sal = get_token_saliency(test_text)\n",
    "\n",
    "print(f\"Token saliency test:\")\n",
    "print(f\"  Total tokens: {len(tokens)}\")\n",
    "print(f\"  Top {len(top_indices)} mutable tokens:\")\n",
    "for idx, token, sal in token_sal[:len(top_indices)]:\n",
    "    print(f\"    [{idx}] {token}: {sal:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79019815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation test:\n",
      "  Original: Technology has become an integral part of modern life.\n",
      "  Mutated:  Technology is a huge part of how we live these days.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Gemini-based Text Mutation\n",
    "def mutate_with_gemini(text, mutation_type=\"rhythm\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Use Gemini to mutate text while preserving meaning.\n",
    "    \n",
    "    Args:\n",
    "        text: Original text\n",
    "        mutation_type: Type of mutation (rhythm, vocabulary, structure, irregularity)\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        mutated_text: Modified text\n",
    "    \"\"\"\n",
    "    mutation_prompts = {\n",
    "        \"rhythm\": \"slightly change the sentence rhythm and flow\",\n",
    "        \"vocabulary\": \"swap some words with natural synonyms\",\n",
    "        \"structure\": \"adjust sentence structure while keeping meaning\",\n",
    "        \"irregularity\": \"introduce subtle stylistic irregularities that sound human\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"Rewrite this paragraph to {mutation_prompts.get(mutation_type, mutation_prompts['rhythm'])} while preserving the core meaning.\n",
    "\n",
    "Original paragraph:\n",
    "{text}\n",
    "\n",
    "Rules:\n",
    "- Keep the main message and information intact\n",
    "- Make the writing feel more natural and less polished\n",
    "- Do NOT add spelling errors or break grammar\n",
    "- Avoid words like: tapestry, delve, testament, integral, unprecedented\n",
    "- Output ONLY the rewritten paragraph, nothing else\n",
    "\n",
    "Rewritten paragraph:\"\"\".strip()\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": 0.95,\n",
    "                \"max_output_tokens\": 8192,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            mutated = response.candidates[0].content.parts[0].text.strip()\n",
    "            # Remove quotes if present\n",
    "            if mutated.startswith('\"') and mutated.endswith('\"'):\n",
    "                mutated = mutated[1:-1]\n",
    "            return mutated\n",
    "        else:\n",
    "            return text  # Return original if generation fails\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Mutation error: {e}\")\n",
    "        return text\n",
    "\n",
    "# Test\n",
    "test_text = \"Technology has become an integral part of modern life.\"\n",
    "mutated = mutate_with_gemini(test_text, \"rhythm\")\n",
    "print(f\"Mutation test:\")\n",
    "print(f\"  Original: {test_text}\")\n",
    "print(f\"  Mutated:  {mutated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe21e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Generate Initial Population\n",
    "def generate_initial_population(original_text, population_size=10):\n",
    "    \"\"\"\n",
    "    Generate initial population from original text.\n",
    "    - First individual: original text (copied to population/1.txt)\n",
    "    - Rest: Gemini-generated variations with high temperature\n",
    "    \n",
    "    Args:\n",
    "        original_text: The source text from input.txt\n",
    "        population_size: Total population size (default 10)\n",
    "    \n",
    "    Returns:\n",
    "        population: List of text candidates\n",
    "    \"\"\"\n",
    "    print(f\"Generating initial population of {population_size}...\")\n",
    "    \n",
    "    # Clear population directory\n",
    "    if POPULATION_DIR.exists():\n",
    "        shutil.rmtree(POPULATION_DIR)\n",
    "    POPULATION_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    population = []\n",
    "    \n",
    "    # First individual: original text\n",
    "    population.append(original_text)\n",
    "    with open(POPULATION_DIR / \"1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(original_text)\n",
    "    print(f\"  [1/{population_size}] Original text saved\")\n",
    "    \n",
    "    # Generate variations with high diversity\n",
    "    prompt_template = \"\"\"Generate a paragraph that is SIMILAR in meaning to this one, but written differently:\n",
    "\n",
    "{text}\n",
    "\n",
    "Requirements:\n",
    "- Keep the same core message and topic\n",
    "- Write it in a slightly different style or voice\n",
    "- Change sentence structure and word choices\n",
    "- Make it sound natural and fluent\n",
    "- {words_min}-{words_max} words\n",
    "- Output ONLY the paragraph, nothing else\n",
    "\n",
    "New paragraph:\"\"\"\n",
    "\n",
    "    word_count = len(original_text.split())\n",
    "    words_min = max(50, int(word_count * 0.8))\n",
    "    words_max = int(word_count * 1.2)\n",
    "    \n",
    "    for i in range(2, population_size + 1):\n",
    "        prompt = prompt_template.format(\n",
    "            text=original_text,\n",
    "            words_min=words_min,\n",
    "            words_max=words_max\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=prompt,\n",
    "                config={\n",
    "                    \"temperature\": 1.0,  # High diversity\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"max_output_tokens\": 8192,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if response.candidates and response.candidates[0].content.parts:\n",
    "                new_text = response.candidates[0].content.parts[0].text.strip()\n",
    "                # Remove quotes if present\n",
    "                if new_text.startswith('\"') and new_text.endswith('\"'):\n",
    "                    new_text = new_text[1:-1]\n",
    "                \n",
    "                population.append(new_text)\n",
    "                \n",
    "                # Save to file\n",
    "                with open(POPULATION_DIR / f\"{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(new_text)\n",
    "                \n",
    "                print(f\"  [{i}/{population_size}] Generated variation\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                # Debug: Show why generation failed\n",
    "                print(f\"  [{i}/{population_size}] Generation failed!\")\n",
    "                if response.candidates:\n",
    "                    print(f\"      Finish reason: {response.candidates[0].finish_reason}\")\n",
    "                    if hasattr(response.candidates[0], 'safety_ratings'):\n",
    "                        print(f\"      Safety ratings: {response.candidates[0].safety_ratings}\")\n",
    "                else:\n",
    "                    print(f\"      No candidates in response\")\n",
    "                print(f\"      Using original text instead\")\n",
    "                population.append(original_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  [{i}/{population_size}] Error: {e}, using original\")\n",
    "            population.append(original_text)\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(f\"Initial population generated: {len(population)} individuals\")\n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09012e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Local Search with Simulated Annealing\n",
    "def local_search(text, original_text, max_iterations=5, initial_temp=0.1):\n",
    "    \"\"\"\n",
    "    Perform local search on a candidate text.\n",
    "    Uses simulated annealing to escape local optima.\n",
    "    \n",
    "    Args:\n",
    "        text: Current candidate text\n",
    "        original_text: Original text for fitness calculation\n",
    "        max_iterations: Number of local search steps\n",
    "        initial_temp: Initial temperature for simulated annealing\n",
    "    \n",
    "    Returns:\n",
    "        best_text: Improved text\n",
    "        best_fitness: Fitness of best text\n",
    "    \"\"\"\n",
    "    current_text = text\n",
    "    current_fitness, _ = calculate_fitness(current_text, original_text)\n",
    "    best_text = current_text\n",
    "    best_fitness = current_fitness\n",
    "    \n",
    "    mutation_types = [\"rhythm\", \"vocabulary\", \"structure\", \"irregularity\"]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Temperature decay\n",
    "        temp = initial_temp * (0.8 ** iteration)\n",
    "        \n",
    "        # Try mutation\n",
    "        mutation_type = np.random.choice(mutation_types)\n",
    "        candidate = mutate_with_gemini(current_text, mutation_type, temperature=0.7)\n",
    "        candidate_fitness, _ = calculate_fitness(candidate, original_text)\n",
    "        \n",
    "        # Accept if better, or with probability based on temperature\n",
    "        delta = candidate_fitness - current_fitness\n",
    "        if delta > 0 or np.random.random() < np.exp(delta / temp):\n",
    "            current_text = candidate\n",
    "            current_fitness = candidate_fitness\n",
    "            \n",
    "            # Update best\n",
    "            if current_fitness > best_fitness:\n",
    "                best_text = current_text\n",
    "                best_fitness = current_fitness\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return best_text, best_fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6aa98c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evolution operators defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Evolution Operators\n",
    "def selection(population, fitnesses, top_k=3):\n",
    "    \"\"\"Select top-k individuals.\"\"\"\n",
    "    sorted_indices = np.argsort(fitnesses)[::-1]\n",
    "    selected_indices = sorted_indices[:top_k]\n",
    "    selected = [population[i] for i in selected_indices]\n",
    "    selected_fitnesses = [fitnesses[i] for i in selected_indices]\n",
    "    return selected, selected_fitnesses, selected_indices\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Crossover two parents by blending their styles.\n",
    "    Uses Gemini to create a hybrid.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Create a new paragraph that blends the writing style of these two paragraphs while keeping similar meaning:\n",
    "\n",
    "Paragraph 1:\n",
    "{parent1}\n",
    "\n",
    "Paragraph 2:\n",
    "{parent2}\n",
    "\n",
    "Requirements:\n",
    "- Combine stylistic elements from both paragraphs\n",
    "- Maintain coherent meaning\n",
    "- Create something that feels like a natural blend\n",
    "- Output ONLY the blended paragraph\n",
    "\n",
    "Blended paragraph:\"\"\".strip()\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 0.95,\n",
    "                \"max_output_tokens\": 8192,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            child = response.candidates[0].content.parts[0].text.strip()\n",
    "            if child.startswith('\"') and child.endswith('\"'):\n",
    "                child = child[1:-1]\n",
    "            return child\n",
    "        else:\n",
    "            return parent1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Crossover error: {e}\")\n",
    "        return parent1\n",
    "\n",
    "def mutate(text, mutation_rate=0.3):\n",
    "    \"\"\"\n",
    "    Mutate text with given probability.\n",
    "    \"\"\"\n",
    "    if np.random.random() < mutation_rate:\n",
    "        mutation_type = np.random.choice([\"rhythm\", \"vocabulary\", \"structure\", \"irregularity\"])\n",
    "        return mutate_with_gemini(text, mutation_type)\n",
    "    return text\n",
    "\n",
    "print(\"Evolution operators defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90cd3511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Log Generation to File\n",
    "def log_generation(generation_num, population, fitnesses, original_text, filepath=None):\n",
    "    \"\"\"\n",
    "    Log generation details to file.\n",
    "    \n",
    "    Args:\n",
    "        generation_num: Generation number\n",
    "        population: List of candidate texts\n",
    "        fitnesses: List of fitness scores\n",
    "        original_text: Original text for reference\n",
    "        filepath: Optional custom filepath\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = EVOLUTION_DIR / f\"evolution_{generation_num}.txt\"\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"GENERATION {generation_num}\\n\")\n",
    "        \n",
    "        # Sort by fitness\n",
    "        sorted_indices = np.argsort(fitnesses)[::-1]\n",
    "        \n",
    "        for rank, idx in enumerate(sorted_indices, 1):\n",
    "            text = population[idx]\n",
    "            fitness = fitnesses[idx]\n",
    "            \n",
    "            # Get detailed metrics\n",
    "            _, components = calculate_fitness(text, original_text)\n",
    "            probs, pred_class, conf = get_classifier_predictions(text)\n",
    "            \n",
    "            f.write(f\"Rank {rank} | Individual {idx + 1}\\n\\n\")\n",
    "            f.write(f\"Fitness: {fitness:.4f}\\n\")\n",
    "            f.write(f\"  - P(Human): {components['p_human']:.4f}\\n\")\n",
    "            f.write(f\"  - P(AI): {probs[1]:.4f}\\n\")\n",
    "            f.write(f\"  - P(AI-mimicry): {probs[2]:.4f}\\n\")\n",
    "            f.write(f\"  - Semantic similarity: {components['similarity']:.4f}\\n\")\n",
    "            f.write(f\"  - Perplexity: {components['perplexity']:.4f}\\n\")\n",
    "            f.write(f\"  - Predicted class: {pred_class+1} (conf={conf:.4f})\\n\")\n",
    "            f.write(f\"\\nText:\\n{text}\\n\")\n",
    "            f.write(f\"\\n{'=' * 80}\\n\\n\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        f.write(f\"\\nSUMMARY STATISTICS\\n\")\n",
    "        f.write(f\"Best fitness: {max(fitnesses):.4f}\\n\")\n",
    "        f.write(f\"Avg fitness:  {np.mean(fitnesses):.4f}\\n\")\n",
    "        f.write(f\"Worst fitness: {min(fitnesses):.4f}\\n\")\n",
    "        f.write(f\"Std dev:      {np.std(fitnesses):.4f}\\n\")\n",
    "    \n",
    "    print(f\"Logged to {filepath.name}\")\n",
    "\n",
    "print(\"Logging function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "713b4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Main MATE Algorithm\n",
    "def run_mate(original_text, num_generations, population_size=10, top_k_parents=3):\n",
    "    \"\"\"\n",
    "    Run the Memetic Algorithm for Text Evolution (MATE).\n",
    "    \n",
    "    Args:\n",
    "        original_text: Source text to evolve\n",
    "        num_generations: Number of generations to run\n",
    "        population_size: Size of population\n",
    "        top_k_parents: Number of parents to select\n",
    "    \n",
    "    Returns:\n",
    "        best_individual: Best text found\n",
    "        best_fitness: Fitness of best text\n",
    "        history: Evolution history\n",
    "    \"\"\"\n",
    "    print(\"=== MEMETIC ALGORITHM FOR TEXT EVOLUTION (MATE) ===\")\n",
    "    print(\"  FITNESS IN LOG SPACE - Expect negative values!\")\n",
    "    print(\"  ELITISM DISABLED - Will re-enable after signal stabilizes\\n\")\n",
    "    \n",
    "    # Setup evolution directory\n",
    "    if EVOLUTION_DIR.exists():\n",
    "        shutil.rmtree(EVOLUTION_DIR)\n",
    "    EVOLUTION_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize population\n",
    "    print(\"\\n[STEP 1] Initializing population...\")\n",
    "    population = generate_initial_population(original_text, population_size)\n",
    "    \n",
    "    # Evaluate initial population\n",
    "    print(\"\\n[STEP 2] Evaluating initial population...\")\n",
    "    fitnesses = []\n",
    "    for i, text in enumerate(population, 1):\n",
    "        fitness, comp = calculate_fitness(text, original_text)\n",
    "        fitnesses.append(fitness)\n",
    "        probs, _, _ = get_classifier_predictions(text)\n",
    "        # HIGH PRECISION logging to see tiny differences\n",
    "        print(f\"  [{i}/{population_size}] Fitness={fitness:.8f}, P(Human)={probs[0]:.8f}\")\n",
    "        print(f\"      ├─ log(P(Human))={comp['log_p_human']:.8f}\")\n",
    "        print(f\"      ├─ Semantic penalty={comp['semantic_penalty']:.8f}\")\n",
    "        print(f\"      └─ Perplexity penalty={comp['perplexity_penalty']:.8f}\")\n",
    "    \n",
    "    # Log generation 0\n",
    "    log_generation(0, population, fitnesses, original_text)\n",
    "    \n",
    "    # Track best ever\n",
    "    best_fitness = max(fitnesses)\n",
    "    best_individual = population[np.argmax(fitnesses)]\n",
    "    history = []\n",
    "    \n",
    "    print(f\"\\n✓ Initial best fitness: {best_fitness:.8f}\")\n",
    "    \n",
    "    # Evolution loop\n",
    "    for gen in range(1, num_generations + 1):\n",
    "        print(f\"\\nGENERATION {gen}/{num_generations}\")\n",
    "        \n",
    "        # Selection\n",
    "        print(\"\\n[STEP 3] Selection...\")\n",
    "        parents, parent_fitnesses, parent_indices = selection(population, fitnesses, top_k_parents)\n",
    "        print(f\"  Selected top {len(parents)} parents\")\n",
    "        for i, (p_idx, fit) in enumerate(zip(parent_indices, parent_fitnesses), 1):\n",
    "            probs, _, _ = get_classifier_predictions(parents[i-1])\n",
    "            _, comp = calculate_fitness(parents[i-1], original_text)\n",
    "            print(f\"    Parent {i} (idx={p_idx+1}): Fitness={fit:.8f}, P(Human)={probs[0]:.8f}\")\n",
    "            print(f\"        └─ Components: log(P)={comp['log_p_human']:.6f}, sem_pen={comp['semantic_penalty']:.6f}, ppl_pen={comp['perplexity_penalty']:.6f}\")\n",
    "        \n",
    "        # Create offspring via crossover and mutation\n",
    "        print(\"\\n[STEP 4] Crossover & Mutation...\")\n",
    "        offspring = []\n",
    "        \n",
    "        # ELITISM TEMPORARILY DISABLED for debugging\n",
    "        # When fitness signal is saturated, elitism clones dead individuals\n",
    "        # Re-enable after fitness starts separating\n",
    "        # offspring.append(parents[0])\n",
    "        # print(f\"  Elitism: Keeping best parent\")\n",
    "        print(f\"  Elitism: DISABLED (debug mode - fitness signal needs to stabilize first)\")\n",
    "        \n",
    "        # Generate ALL offspring through crossover + mutation (no elitism)\n",
    "        while len(offspring) < population_size:\n",
    "            # Select two random parents\n",
    "            p1, p2 = np.random.choice(len(parents), size=2, replace=False)\n",
    "            \n",
    "            # Crossover\n",
    "            child = crossover(parents[p1], parents[p2])\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Mutation\n",
    "            child = mutate(child, mutation_rate=0.3)\n",
    "            \n",
    "            offspring.append(child)\n",
    "            print(f\"  Generated offspring {len(offspring)}/{population_size}\")\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Local search on top 3 offspring\n",
    "        print(\"\\n[STEP 5] Local search on top offspring...\")\n",
    "        offspring_fitnesses = [calculate_fitness(text, original_text)[0] for text in offspring]\n",
    "        top_offspring_indices = np.argsort(offspring_fitnesses)[::-1][:3]\n",
    "        \n",
    "        for i, idx in enumerate(top_offspring_indices, 1):\n",
    "            print(f\"  Local search on offspring {idx+1}...\")\n",
    "            improved_text, improved_fitness = local_search(\n",
    "                offspring[idx], \n",
    "                original_text, \n",
    "                max_iterations=3\n",
    "            )\n",
    "            offspring[idx] = improved_text\n",
    "            offspring_fitnesses[idx] = improved_fitness\n",
    "            print(f\"    Fitness: {offspring_fitnesses[idx]:.8f}\")\n",
    "        \n",
    "        # Replace population\n",
    "        population = offspring\n",
    "        fitnesses = offspring_fitnesses\n",
    "        \n",
    "        # Update best\n",
    "        gen_best_fitness = max(fitnesses)\n",
    "        gen_best_individual = population[np.argmax(fitnesses)]\n",
    "        \n",
    "        if gen_best_fitness > best_fitness:\n",
    "            best_fitness = gen_best_fitness\n",
    "            best_individual = gen_best_individual\n",
    "            print(f\"\\nNEW BEST FITNESS: {best_fitness:.8f}\")\n",
    "        \n",
    "        # Log generation\n",
    "        log_generation(gen, population, fitnesses, original_text)\n",
    "        \n",
    "        # Statistics\n",
    "        history.append({\n",
    "            'generation': gen,\n",
    "            'best_fitness': gen_best_fitness,\n",
    "            'avg_fitness': np.mean(fitnesses),\n",
    "            'worst_fitness': min(fitnesses),\n",
    "            'best_p_human': get_human_probability(gen_best_individual)\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n  Generation {gen} summary (HIGH PRECISION):\")\n",
    "        print(f\"    Best fitness:  {gen_best_fitness:.8f}\")\n",
    "        print(f\"    Avg fitness:   {np.mean(fitnesses):.8f}\")\n",
    "        print(f\"    Worst fitness: {min(fitnesses):.8f}\")\n",
    "        print(f\"    Best P(Human): {get_human_probability(gen_best_individual):.8f}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVOLUTION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBest fitness achieved: {best_fitness:.8f}\")\n",
    "    probs, pred_class, conf = get_classifier_predictions(best_individual)\n",
    "    print(f\"Best individual:\")\n",
    "    print(f\"  P(Human): {probs[0]:.8f}\")\n",
    "    print(f\"  P(AI): {probs[1]:.8f}\")\n",
    "    print(f\"  P(AI-mimicry): {probs[2]:.8f}\")\n",
    "    print(f\"  Predicted: Class {pred_class+1} (conf={conf:.4f})\")\n",
    "    print(f\"\\nText:\\n{best_individual}\")\n",
    "    \n",
    "    return best_individual, best_fitness, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b1c93f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input text...\n",
      " Input text loaded (154 words)\n",
      "\n",
      "Original text:\n",
      "The quiet hum of this neighborhood always felt like safety, a soft blanket of routine where the biggest scandal was usually a zoning dispute or an unkempt lawn. I honestly believed I knew the man living in the yellow bungalow across the street. He was the sort of person who returned borrowed tools early and bought Girl Scout cookies by the case, practically invisible in his decency. That illusion shattered the afternoon I accidentally opened his mail, a simple mix-up by the postman. The letter wasn't a utility bill or a birthday card; it was a heavily redacted government dossier with a photo of him looking twenty years younger and terrifyingly cold. Standing there on my porch, holding the envelope that felt suddenly heavy, I realized his gentle demeanor wasn't a personality trait. It was a camouflage, perfect and impenetrable, hiding a history that had no business existing between the library and the bakery.\n",
      "\n",
      "Initial classification:\n",
      "  P(Human): 0.0000\n",
      "  P(AI): 1.0000\n",
      "  P(AI-mimicry): 0.0000\n",
      "  Predicted: Class 2 (conf=1.0000)\n",
      "\n",
      "Will run 5 generations\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: RUN MATE - Configuration\n",
    "# Read input text\n",
    "print(\"Reading input text...\")\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_text = f.read().strip()\n",
    "\n",
    "if not original_text:\n",
    "    print(\"WARNING: input.txt is empty!\")\n",
    "    print(\"Please add your AI-generated paragraph to input.txt and run this cell again.\")\n",
    "else:\n",
    "    print(f\" Input text loaded ({len(original_text.split())} words)\")\n",
    "    print(f\"\\nOriginal text:\")\n",
    "    print(f\"{original_text}\")\n",
    "    \n",
    "    # Get initial prediction\n",
    "    probs, pred_class, conf = get_classifier_predictions(original_text)\n",
    "    print(f\"\\nInitial classification:\")\n",
    "    print(f\"  P(Human): {probs[0]:.4f}\")\n",
    "    print(f\"  P(AI): {probs[1]:.4f}\")\n",
    "    print(f\"  P(AI-mimicry): {probs[2]:.4f}\")\n",
    "    print(f\"  Predicted: Class {pred_class+1} (conf={conf:.4f})\")\n",
    "    \n",
    "    # Set number of generations\n",
    "    num_generations = int(input(\"\\nEnter number of generations to run: \"))\n",
    "    print(f\"\\nWill run {num_generations} generations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235e900",
   "metadata": {},
   "source": [
    "# Some Important Terms\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c602aea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MEMETIC ALGORITHM FOR TEXT EVOLUTION (MATE) ===\n",
      "  FITNESS IN LOG SPACE - Expect negative values!\n",
      "  ELITISM DISABLED - Will re-enable after signal stabilizes\n",
      "\n",
      "\n",
      "[STEP 1] Initializing population...\n",
      "Generating initial population of 10...\n",
      "  [1/10] Original text saved\n",
      "  [2/10] Generated variation\n",
      "  [3/10] Generated variation\n",
      "  [4/10] Generated variation\n",
      "  [5/10] Generated variation\n",
      "  [6/10] Generated variation\n",
      "  [7/10] Generated variation\n",
      "  [8/10] Generated variation\n",
      "  [9/10] Generated variation\n",
      "  [10/10] Generated variation\n",
      "✓ Initial population generated: 10 individuals\n",
      "\n",
      "[STEP 2] Evaluating initial population...\n",
      "  [1/10] Fitness=-14.04408646, P(Human)=0.00000080\n",
      "      ├─ log(P(Human))=-14.04408646\n",
      "      ├─ Semantic penalty=0.00000000\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [2/10] Fitness=-14.26171398, P(Human)=0.00000070\n",
      "      ├─ log(P(Human))=-14.17290115\n",
      "      ├─ Semantic penalty=0.08881271\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [3/10] Fitness=-13.46456242, P(Human)=0.00000172\n",
      "      ├─ log(P(Human))=-13.27543831\n",
      "      ├─ Semantic penalty=0.18912411\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [4/10] Fitness=-14.15458870, P(Human)=0.00000075\n",
      "      ├─ log(P(Human))=-14.09905338\n",
      "      ├─ Semantic penalty=0.05553532\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [5/10] Fitness=-14.71243858, P(Human)=0.00000047\n",
      "      ├─ log(P(Human))=-14.56390762\n",
      "      ├─ Semantic penalty=0.14853084\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [6/10] Fitness=-14.18745232, P(Human)=0.00000081\n",
      "      ├─ log(P(Human))=-14.03028393\n",
      "      ├─ Semantic penalty=0.15716839\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [7/10] Fitness=-13.85217285, P(Human)=0.00000103\n",
      "      ├─ log(P(Human))=-13.78927803\n",
      "      ├─ Semantic penalty=0.06289482\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [8/10] Fitness=-13.48809242, P(Human)=0.00000153\n",
      "      ├─ log(P(Human))=-13.39238548\n",
      "      ├─ Semantic penalty=0.09570730\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [9/10] Fitness=-14.20878983, P(Human)=0.00000069\n",
      "      ├─ log(P(Human))=-14.18442631\n",
      "      ├─ Semantic penalty=0.02436376\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "  [10/10] Fitness=-14.72011662, P(Human)=0.00000046\n",
      "      ├─ log(P(Human))=-14.60155296\n",
      "      ├─ Semantic penalty=0.11856365\n",
      "      └─ Perplexity penalty=0.00000000\n",
      "Logged to evolution_0.txt\n",
      "\n",
      "✓ Initial best fitness: -13.46456242\n",
      "\n",
      "GENERATION 1/5\n",
      "\n",
      "[STEP 3] Selection...\n",
      "  Selected top 3 parents\n",
      "    Parent 1 (idx=3): Fitness=-13.46456242, P(Human)=0.00000172\n",
      "        └─ Components: log(P)=-13.275438, sem_pen=0.189124, ppl_pen=0.000000\n",
      "    Parent 2 (idx=8): Fitness=-13.48809242, P(Human)=0.00000153\n",
      "        └─ Components: log(P)=-13.392385, sem_pen=0.095707, ppl_pen=0.000000\n",
      "    Parent 3 (idx=7): Fitness=-13.85217285, P(Human)=0.00000103\n",
      "        └─ Components: log(P)=-13.789278, sem_pen=0.062895, ppl_pen=0.000000\n",
      "\n",
      "[STEP 4] Crossover & Mutation...\n",
      "  Elitism: DISABLED (debug mode - fitness signal needs to stabilize first)\n",
      "  Generated offspring 1/10\n",
      "  Generated offspring 2/10\n",
      "  Generated offspring 3/10\n",
      "  Generated offspring 4/10\n",
      "  Generated offspring 5/10\n",
      "  Generated offspring 6/10\n",
      "  Generated offspring 7/10\n",
      "  Generated offspring 8/10\n",
      "  Generated offspring 9/10\n",
      "  Generated offspring 10/10\n",
      "\n",
      "[STEP 5] Local search on top offspring...\n",
      "  Local search on offspring 2...\n",
      "    Fitness: -13.20361900\n",
      "  Local search on offspring 3...\n",
      "    Fitness: -13.27826977\n",
      "  Local search on offspring 9...\n",
      "    Fitness: -13.39863777\n",
      "\n",
      "NEW BEST FITNESS: -13.20361900\n",
      "Logged to evolution_1.txt\n",
      "\n",
      "  Generation 1 summary (HIGH PRECISION):\n",
      "    Best fitness:  -13.20361900\n",
      "    Avg fitness:   -14.04334259\n",
      "    Worst fitness: -14.90683174\n",
      "    Best P(Human): 0.00000205\n",
      "\n",
      "GENERATION 2/5\n",
      "\n",
      "[STEP 3] Selection...\n",
      "  Selected top 3 parents\n",
      "    Parent 1 (idx=2): Fitness=-13.20361900, P(Human)=0.00000205\n",
      "        └─ Components: log(P)=-13.099806, sem_pen=0.103813, ppl_pen=0.000000\n",
      "    Parent 2 (idx=3): Fitness=-13.27826977, P(Human)=0.00000205\n",
      "        └─ Components: log(P)=-13.098353, sem_pen=0.179916, ppl_pen=0.000000\n",
      "    Parent 3 (idx=9): Fitness=-13.39863777, P(Human)=0.00000165\n",
      "        └─ Components: log(P)=-13.317354, sem_pen=0.081283, ppl_pen=0.000000\n",
      "\n",
      "[STEP 4] Crossover & Mutation...\n",
      "  Elitism: DISABLED (debug mode - fitness signal needs to stabilize first)\n",
      "  Generated offspring 1/10\n",
      "  Generated offspring 2/10\n",
      "  Generated offspring 3/10\n",
      "  Generated offspring 4/10\n",
      "  Generated offspring 5/10\n",
      "  Generated offspring 6/10\n",
      "  Generated offspring 7/10\n",
      "  Generated offspring 8/10\n",
      "  Generated offspring 9/10\n",
      "  Generated offspring 10/10\n",
      "\n",
      "[STEP 5] Local search on top offspring...\n",
      "  Local search on offspring 3...\n",
      "    Fitness: -13.04807758\n",
      "  Local search on offspring 2...\n",
      "    Fitness: -13.26416779\n",
      "  Local search on offspring 5...\n",
      "    Fitness: -13.27094555\n",
      "\n",
      "NEW BEST FITNESS: -13.04807758\n",
      "Logged to evolution_2.txt\n",
      "\n",
      "  Generation 2 summary (HIGH PRECISION):\n",
      "    Best fitness:  -13.04807758\n",
      "    Avg fitness:   -13.63637829\n",
      "    Worst fitness: -14.19784737\n",
      "    Best P(Human): 0.00000263\n",
      "\n",
      "GENERATION 3/5\n",
      "\n",
      "[STEP 3] Selection...\n",
      "  Selected top 3 parents\n",
      "    Parent 1 (idx=3): Fitness=-13.04807758, P(Human)=0.00000263\n",
      "        └─ Components: log(P)=-12.849195, sem_pen=0.198883, ppl_pen=0.000000\n",
      "    Parent 2 (idx=2): Fitness=-13.26416779, P(Human)=0.00000215\n",
      "        └─ Components: log(P)=-13.051438, sem_pen=0.212729, ppl_pen=0.000000\n",
      "    Parent 3 (idx=5): Fitness=-13.27094555, P(Human)=0.00000216\n",
      "        └─ Components: log(P)=-13.043862, sem_pen=0.227083, ppl_pen=0.000000\n",
      "\n",
      "[STEP 4] Crossover & Mutation...\n",
      "  Elitism: DISABLED (debug mode - fitness signal needs to stabilize first)\n",
      "  Generated offspring 1/10\n",
      "  Generated offspring 2/10\n",
      "  Generated offspring 3/10\n",
      "  Generated offspring 4/10\n",
      "  Generated offspring 5/10\n",
      "  Generated offspring 6/10\n",
      "  Generated offspring 7/10\n",
      "  Generated offspring 8/10\n",
      "  Generated offspring 9/10\n",
      "  Generated offspring 10/10\n",
      "\n",
      "[STEP 5] Local search on top offspring...\n",
      "  Local search on offspring 1...\n",
      "    Fitness: -13.06719303\n",
      "  Local search on offspring 3...\n",
      "    Fitness: -13.07224941\n",
      "  Local search on offspring 7...\n",
      "    Fitness: -13.13153648\n",
      "Logged to evolution_3.txt\n",
      "\n",
      "  Generation 3 summary (HIGH PRECISION):\n",
      "    Best fitness:  -13.06719303\n",
      "    Avg fitness:   -13.45651722\n",
      "    Worst fitness: -14.47307205\n",
      "    Best P(Human): 0.00000264\n",
      "\n",
      "GENERATION 4/5\n",
      "\n",
      "[STEP 3] Selection...\n",
      "  Selected top 3 parents\n",
      "    Parent 1 (idx=1): Fitness=-13.06719303, P(Human)=0.00000264\n",
      "        └─ Components: log(P)=-12.846405, sem_pen=0.220788, ppl_pen=0.000000\n",
      "    Parent 2 (idx=3): Fitness=-13.07224941, P(Human)=0.00000257\n",
      "        └─ Components: log(P)=-12.871972, sem_pen=0.200277, ppl_pen=0.000000\n",
      "    Parent 3 (idx=7): Fitness=-13.13153648, P(Human)=0.00000249\n",
      "        └─ Components: log(P)=-12.904545, sem_pen=0.226992, ppl_pen=0.000000\n",
      "\n",
      "[STEP 4] Crossover & Mutation...\n",
      "  Elitism: DISABLED (debug mode - fitness signal needs to stabilize first)\n",
      "  Generated offspring 1/10\n",
      "  Generated offspring 2/10\n",
      "  Generated offspring 3/10\n",
      "  Generated offspring 4/10\n",
      "  Generated offspring 5/10\n",
      "  Generated offspring 6/10\n",
      "  Generated offspring 7/10\n",
      "  Generated offspring 8/10\n",
      "  Generated offspring 9/10\n",
      "  Generated offspring 10/10\n",
      "\n",
      "[STEP 5] Local search on top offspring...\n",
      "  Local search on offspring 6...\n",
      "    Fitness: -13.00923634\n",
      "  Local search on offspring 8...\n",
      "    Fitness: -13.04746532\n",
      "  Local search on offspring 1...\n",
      "    Fitness: -13.04746532\n",
      "\n",
      "NEW BEST FITNESS: -13.00923634\n",
      "Logged to evolution_4.txt\n",
      "\n",
      "  Generation 4 summary (HIGH PRECISION):\n",
      "    Best fitness:  -13.00923634\n",
      "    Avg fitness:   -13.52441120\n",
      "    Worst fitness: -14.76366234\n",
      "    Best P(Human): 0.00000276\n",
      "\n",
      "GENERATION 5/5\n",
      "\n",
      "[STEP 3] Selection...\n",
      "  Selected top 3 parents\n",
      "    Parent 1 (idx=6): Fitness=-13.00923634, P(Human)=0.00000276\n",
      "        └─ Components: log(P)=-12.800935, sem_pen=0.208302, ppl_pen=0.000000\n",
      "    Parent 2 (idx=8): Fitness=-13.04746532, P(Human)=0.00000262\n",
      "        └─ Components: log(P)=-12.851330, sem_pen=0.196135, ppl_pen=0.000000\n",
      "    Parent 3 (idx=1): Fitness=-13.04746532, P(Human)=0.00000262\n",
      "        └─ Components: log(P)=-12.851330, sem_pen=0.196135, ppl_pen=0.000000\n",
      "\n",
      "[STEP 4] Crossover & Mutation...\n",
      "  Elitism: DISABLED (debug mode - fitness signal needs to stabilize first)\n",
      "  Generated offspring 1/10\n",
      "  Generated offspring 2/10\n",
      "  Generated offspring 3/10\n",
      "  Generated offspring 4/10\n",
      "  Generated offspring 5/10\n",
      "  Generated offspring 6/10\n",
      "  Generated offspring 7/10\n",
      "  Generated offspring 8/10\n",
      "  Generated offspring 9/10\n",
      "  Generated offspring 10/10\n",
      "\n",
      "[STEP 5] Local search on top offspring...\n",
      "  Local search on offspring 3...\n",
      "    Fitness: -12.46549320\n",
      "  Local search on offspring 2...\n",
      "    Fitness: -12.93619728\n",
      "  Local search on offspring 1...\n",
      "    Fitness: -12.96134853\n",
      "\n",
      "NEW BEST FITNESS: -12.46549320\n",
      "Logged to evolution_5.txt\n",
      "\n",
      "  Generation 5 summary (HIGH PRECISION):\n",
      "    Best fitness:  -12.46549320\n",
      "    Avg fitness:   -13.39245892\n",
      "    Worst fitness: -14.22165012\n",
      "    Best P(Human): 0.00000466\n",
      "\n",
      "============================================================\n",
      "EVOLUTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Best fitness achieved: -12.46549320\n",
      "Best individual:\n",
      "  P(Human): 0.00000466\n",
      "  P(AI): 0.98721123\n",
      "  P(AI-mimicry): 0.01278409\n",
      "  Predicted: Class 2 (conf=0.9872)\n",
      "\n",
      "Text:\n",
      "I had always taken deep security from the comforting predictability of my street, a serene world where the greatest drama was a misaimed sprinkler or a potluck sign-up sheet. The man in the yellow house, with his perfectly manicured hedges, was the epitome of the good neighbor—a person whose unremarkable pleasantness I mistakenly believed I understood completely. My perception of him, and of my quiet life, was shattered one Tuesday by a simple postal error. The thick manila envelope I tore open contained no mundane mail, but a classified dossier, its pages a sea of black redactions. Clipped within was a photograph of him, two decades younger, his eyes holding a lethally sharp coldness that felt utterly alien. That envelope, suddenly a dead weight in my hand, revealed the truth: his harmless, mild-mannered persona was no accident but a disciplined performance, a meticulously built facade to conceal a past forged in a darker world, one that had no place among our block parties and bake sales.\n",
      "\n",
      "Best individual saved to best_individual.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: RUN MATE - Execute Evolution\n",
    "# Run MATE algorithm\n",
    "best_individual, best_fitness, history = run_mate(\n",
    "    original_text=original_text,\n",
    "    num_generations=num_generations,\n",
    "    population_size=10,\n",
    "    top_k_parents=3\n",
    ")\n",
    "\n",
    "# Save best individual\n",
    "best_file = Path(\"best_individual.txt\")\n",
    "with open(best_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Best Individual from MATE Evolution\\n\")\n",
    "    f.write(f\"Fitness: {best_fitness:.4f}\\n\\n\")\n",
    "    \n",
    "    probs, pred_class, conf = get_classifier_predictions(best_individual)\n",
    "    f.write(f\"Classification:\\n\")\n",
    "    f.write(f\"  P(Human): {probs[0]:.4f}\\n\")\n",
    "    f.write(f\"  P(AI): {probs[1]:.4f}\\n\")\n",
    "    f.write(f\"  P(AI-mimicry): {probs[2]:.4f}\\n\")\n",
    "    f.write(f\"  Predicted: Class {pred_class} (conf={conf:.4f})\\n\\n\")\n",
    "    f.write(f\"Text:\\n{best_individual}\\n\")\n",
    "\n",
    "print(f\"\\nBest individual saved to {best_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6c626",
   "metadata": {},
   "source": [
    "After a few rounds of running cell 17, I kept getting [this output](results/1-always-0.md). This is quite problematic. It makes things meaningless if children are evolving from a parent who itself is 0% human! As such, I realise that while the algorithm may be okay, there are a few things I need to change, because rn the memetic algorithm has nothing to optimize!!! This makes local search / global search / whatever you want to call it, meaningless...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e738c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Visualize Evolution Progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot fitness evolution\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Best and average fitness over generations\n",
    "generations = [h['generation'] for h in history]\n",
    "best_fitnesses = [h['best_fitness'] for h in history]\n",
    "avg_fitnesses = [h['avg_fitness'] for h in history]\n",
    "\n",
    "axes[0].plot(generations, best_fitnesses, 'b-o', label='Best Fitness', linewidth=2)\n",
    "axes[0].plot(generations, avg_fitnesses, 'g--s', label='Avg Fitness', linewidth=2)\n",
    "axes[0].set_xlabel('Generation', fontsize=12)\n",
    "axes[0].set_ylabel('Fitness', fontsize=12)\n",
    "axes[0].set_title('Fitness Evolution Over Generations', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: P(Human) probability over generations\n",
    "p_humans = [h['best_p_human'] for h in history]\n",
    "axes[1].plot(generations, p_humans, 'r-o', label='P(Human) of Best', linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='k', linestyle='--', alpha=0.5, label='Decision Boundary')\n",
    "axes[1].set_xlabel('Generation', fontsize=12)\n",
    "axes[1].set_ylabel('P(Human)', fontsize=12)\n",
    "axes[1].set_title('Human Probability of Best Individual', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mate_evolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualization saved to mate_evolution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7626a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Compare Original vs Best\n",
    "print(\"COMPARISON: ORIGINAL vs BEST EVOLVED\")\n",
    "\n",
    "print(\"\\n[ORIGINAL TEXT]\")\n",
    "print(original_text)\n",
    "print()\n",
    "\n",
    "probs_orig, pred_orig, conf_orig = get_classifier_predictions(original_text)\n",
    "print(f\"Classification:\")\n",
    "print(f\"  P(Human): {probs_orig[0]:.4f}\")\n",
    "print(f\"  P(AI): {probs_orig[1]:.4f}\")\n",
    "print(f\"  P(AI-mimicry): {probs_orig[2]:.4f}\")\n",
    "print(f\"  Predicted: Class {pred_orig+1} (conf={conf_orig:.4f})\")\n",
    "\n",
    "print(\"\\n[BEST EVOLVED TEXT]\")\n",
    "print(best_individual)\n",
    "print()\n",
    "\n",
    "probs_best, pred_best, conf_best = get_classifier_predictions(best_individual)\n",
    "print(f\"Classification:\")\n",
    "print(f\"  P(Human): {probs_best[0]:.4f}\")\n",
    "print(f\"  P(AI): {probs_best[1]:.4f}\")\n",
    "print(f\"  P(AI-mimicry): {probs_best[2]:.4f}\")\n",
    "print(f\"  Predicted: Class {pred_best+1} (conf={conf_best:.4f})\")\n",
    "\n",
    "print(\"\\n[IMPROVEMENTS]\")\n",
    "print(f\"P(Human) change: {probs_orig[0]:.4f} → {probs_best[0]:.4f} (+{probs_best[0] - probs_orig[0]:.4f})\")\n",
    "print(f\"P(AI) change: {probs_orig[1]:.4f} → {probs_best[1]:.4f} ({probs_best[1] - probs_orig[1]:.4f})\")\n",
    "print(f\"Semantic similarity: {get_semantic_similarity(original_text, best_individual):.4f}\")\n",
    "\n",
    "if pred_best == 0:\n",
    "    print(\"\\nSUCCESS: Best individual is classified as HUMAN (Class 1)!\")\n",
    "else:\n",
    "    print(f\"\\nStill classified as Class {pred_best+1}, but P(Human) improved by {(probs_best[0] - probs_orig[0]) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
