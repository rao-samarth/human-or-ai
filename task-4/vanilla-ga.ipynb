{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da39827",
   "metadata": {},
   "source": [
    "# Vanilla Genetic Algorithm for Text Evolution (Baseline)\n",
    "\n",
    "This notebook implements a standard \"vanilla\" GA as a baseline to compare against MATE (Memetic Algorithm for Text Evolution).\n",
    "\n",
    "**Key Differences from MATE:**\n",
    "- **No saliency-guided mutations** - uses random synonym replacement\n",
    "- **No local search** - pure evolutionary operators\n",
    "- **No adaptive parameters** - fixed mutation rate and static penalties\n",
    "- **No gradient information** - completely stochastic\n",
    "\n",
    "This serves to demonstrate the advantage of incorporating domain knowledge (saliency) and local refinement (memetic search) into evolutionary algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0d8d9",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23eb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download WordNet if not already present\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    print(\"Downloading WordNet...\")\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "# Setup directories\n",
    "INPUT_FILE = Path(\"input.txt\")\n",
    "MODEL_PATH = Path(\"../task-2/transformer/tier_c_final_model\")\n",
    "GA_DIR = Path(\"ggs\")  # Vanilla GA logs\n",
    "GA_CSV = Path(\"vanilla_ga.csv\")\n",
    "\n",
    "# Create directories\n",
    "GA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Input: {INPUT_FILE}\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Output: {GA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05961e4a",
   "metadata": {},
   "source": [
    "## Cell 2: Load Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Classifier Model\n",
    "print(\"Loading classifier model...\")\n",
    "\n",
    "# Load PEFT configuration\n",
    "peft_model_id = str(MODEL_PATH)\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "model.eval()\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"  Base: {config.base_model_name_or_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8738a",
   "metadata": {},
   "source": [
    "## Cell 3: Load Semantic Similarity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84829a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Semantic Similarity Model\n",
    "print(\"Loading sentence transformer for semantic similarity...\")\n",
    "\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"  Semantic model loaded\")\n",
    "print(\"  Model: all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db5e8a2",
   "metadata": {},
   "source": [
    "## Cell 4: Helper Functions - Classifier & Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper Functions - Classifier & Semantic Similarity\n",
    "def get_classifier_predictions(text):\n",
    "    \"\"\"\n",
    "    Get classifier predictions for a text.\n",
    "    Returns: (probs, predicted_class, confidence)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    predicted_class = np.argmax(probs)\n",
    "    confidence = probs[predicted_class]\n",
    "    \n",
    "    return probs, predicted_class, confidence\n",
    "\n",
    "def get_human_probability(text):\n",
    "    \"\"\"Get P(Human) = P(Class 1) for a text.\"\"\"\n",
    "    probs, _, _ = get_classifier_predictions(text)\n",
    "    return probs[0]\n",
    "\n",
    "def get_semantic_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two texts using sentence embeddings.\n",
    "    Returns: similarity score in [0, 1]\n",
    "    \"\"\"\n",
    "    embeddings = semantic_model.encode([text1, text2])\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Test\n",
    "test_text = \"Technology has become an integral part of modern life.\"\n",
    "probs, pred_class, conf = get_classifier_predictions(test_text)\n",
    "print(f\"Test prediction:\")\n",
    "print(f\"  P(Human): {probs[0]:.4f}\")\n",
    "print(f\"  P(AI): {probs[1]:.4f}\")\n",
    "print(f\"  P(AI-mimicry): {probs[2]:.4f}\")\n",
    "print(f\"  Predicted: Class {pred_class+1} (conf={conf:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a0aed",
   "metadata": {},
   "source": [
    "## Cell 5: Fitness Function with Static Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Fitness Function with Static Penalty\n",
    "def calculate_fitness(text, original_text, semantic_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Vanilla GA Fitness Function with Static Penalty.\n",
    "    \n",
    "    Objective: Maximize P(Human)\n",
    "    Constraint: Semantic similarity must be >= 0.85\n",
    "    \n",
    "    If similarity < 0.85, apply a massive fixed penalty of -100.\n",
    "    This is MUCH simpler than MATE's Lagrangian relaxation with adaptive weights.\n",
    "    \n",
    "    Args:\n",
    "        text: Candidate text\n",
    "        original_text: Original text for semantic comparison\n",
    "        semantic_threshold: Hard threshold (0.85)\n",
    "    \n",
    "    Returns:\n",
    "        fitness: P(Human) if constraint met, else P(Human) - 100\n",
    "        components: Dict with breakdown\n",
    "    \"\"\"\n",
    "    # Primary objective: maximize P(Human)\n",
    "    p_human = get_human_probability(text)\n",
    "    \n",
    "    # Constraint: Semantic similarity\n",
    "    similarity = get_semantic_similarity(text, original_text)\n",
    "    \n",
    "    # Static penalty - either you meet the constraint or you don't\n",
    "    if similarity < semantic_threshold:\n",
    "        penalty = 100.0  # Massive fixed penalty\n",
    "    else:\n",
    "        penalty = 0.0\n",
    "    \n",
    "    fitness = p_human - penalty\n",
    "    \n",
    "    components = {\n",
    "        'fitness': fitness,\n",
    "        'p_human': p_human,\n",
    "        'similarity': similarity,\n",
    "        'penalty': penalty\n",
    "    }\n",
    "    \n",
    "    return fitness, components\n",
    "\n",
    "# Test\n",
    "original = \"Technology has become an integral part of modern life.\"\n",
    "candidate = \"Tech has become a key part of everyday life.\"\n",
    "\n",
    "fitness, comp = calculate_fitness(candidate, original)\n",
    "print(f\"Fitness test:\")\n",
    "print(f\"  Fitness: {fitness:.4f}\")\n",
    "print(f\"  P(Human): {comp['p_human']:.4f}\")\n",
    "print(f\"  Similarity: {comp['similarity']:.4f}\")\n",
    "print(f\"  Penalty: {comp['penalty']:.4f}\")\n",
    "print(f\"\\nNote: This uses a STATIC penalty unlike MATE's adaptive Lagrangian approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66865e",
   "metadata": {},
   "source": [
    "## Cell 6: Synonym-Based Text Manipulation (Random, No Saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce54087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Synonym-Based Text Manipulation (Random, No Saliency)\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Get synonyms for a word using WordNet.\n",
    "    Returns a list of synonyms (excluding the word itself).\n",
    "    \n",
    "    IMPORTANT: This is RANDOM - no saliency, no gradient information.\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            if synonym.lower() != word.lower():\n",
    "                synonyms.add(synonym)\n",
    "    \n",
    "    return list(synonyms)\n",
    "\n",
    "def replace_random_words(text, replacement_rate=0.15):\n",
    "    \"\"\"\n",
    "    Randomly replace X% of words with synonyms.\n",
    "    \n",
    "    CRITICAL DIFFERENCE FROM MATE:\n",
    "    - This is BLIND replacement - no saliency information\n",
    "    - No gradient-based selection of which words to replace\n",
    "    - Just random sampling\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        replacement_rate: Fraction of words to replace (0.15 = 15%)\n",
    "    \n",
    "    Returns:\n",
    "        Modified text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    num_to_replace = max(1, int(len(words) * replacement_rate))\n",
    "    \n",
    "    # Random indices to replace (no saliency guidance!)\n",
    "    indices_to_replace = random.sample(range(len(words)), \n",
    "                                      min(num_to_replace, len(words)))\n",
    "    \n",
    "    for idx in indices_to_replace:\n",
    "        word = words[idx]\n",
    "        # Remove punctuation for synonym lookup\n",
    "        clean_word = word.strip('.,!?;:\"\\'\"').lower()\n",
    "        \n",
    "        synonyms = get_synonyms(clean_word)\n",
    "        if synonyms:\n",
    "            # Randomly pick a synonym (no fitness evaluation!)\n",
    "            new_word = random.choice(synonyms)\n",
    "            \n",
    "            # Preserve capitalization\n",
    "            if word[0].isupper():\n",
    "                new_word = new_word.capitalize()\n",
    "            \n",
    "            # Preserve punctuation\n",
    "            for punct in '.,!?;:\\'\"':\n",
    "                if word.endswith(punct):\n",
    "                    new_word += punct\n",
    "                    break\n",
    "            \n",
    "            words[idx] = new_word\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Test\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"\\nRandom replacements (no saliency):\")\n",
    "for i in range(3):\n",
    "    mutated = replace_random_words(test_text, replacement_rate=0.3)\n",
    "    print(f\"  {i+1}. {mutated}\")\n",
    "\n",
    "# Show synonyms for a word\n",
    "print(f\"\\nExample synonyms for 'quick': {get_synonyms('quick')[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f924631",
   "metadata": {},
   "source": [
    "## Cell 7: Initialize Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Initialize Population\n",
    "def initialize_population(original_text, population_size=20, mutation_rate=0.15):\n",
    "    \"\"\"\n",
    "    Initialize population by creating random variations.\n",
    "    \n",
    "    DIFFERENCE FROM MATE:\n",
    "    - MATE uses Gemini to generate semantically diverse variations\n",
    "    - Vanilla GA uses simple random synonym replacement\n",
    "    - Much less diverse, more likely to get stuck in local optima\n",
    "    \n",
    "    Args:\n",
    "        original_text: Source text\n",
    "        population_size: Number of individuals (20 for vanilla GA)\n",
    "        mutation_rate: Fraction of words to randomly mutate (0.15)\n",
    "    \n",
    "    Returns:\n",
    "        population: List of text individuals\n",
    "    \"\"\"\n",
    "    print(f\"Initializing population of {population_size} individuals...\")\n",
    "    \n",
    "    population = []\n",
    "    \n",
    "    # First individual: original text\n",
    "    population.append(original_text)\n",
    "    print(f\"  [1/{population_size}] Original text\")\n",
    "    \n",
    "    # Generate variations via random synonym replacement\n",
    "    for i in range(2, population_size + 1):\n",
    "        variant = replace_random_words(original_text, replacement_rate=mutation_rate)\n",
    "        population.append(variant)\n",
    "        print(f\"  [{i}/{population_size}] Random variant\")\n",
    "    \n",
    "    print(f\"Population initialized: {len(population)} individuals\")\n",
    "    return population\n",
    "\n",
    "print(\"Population initialization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afe0dc",
   "metadata": {},
   "source": [
    "## Cell 8: Selection Operator - Tournament Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Selection Operator - Tournament Selection\n",
    "def tournament_selection(population, fitnesses, tournament_size=3):\n",
    "    \"\"\"\n",
    "    Tournament Selection: Pick k random individuals, return the best one.\n",
    "    \n",
    "    This is standard GA selection - no fancy ranking or adaptive pressure.\n",
    "    \n",
    "    Args:\n",
    "        population: List of individuals\n",
    "        fitnesses: List of fitness scores\n",
    "        tournament_size: Number of individuals in tournament (k=3)\n",
    "    \n",
    "    Returns:\n",
    "        Selected individual\n",
    "    \"\"\"\n",
    "    # Randomly pick k individuals\n",
    "    tournament_indices = random.sample(range(len(population)), tournament_size)\n",
    "    \n",
    "    # Find the best one\n",
    "    best_idx = tournament_indices[0]\n",
    "    best_fitness = fitnesses[best_idx]\n",
    "    \n",
    "    for idx in tournament_indices[1:]:\n",
    "        if fitnesses[idx] > best_fitness:\n",
    "            best_idx = idx\n",
    "            best_fitness = fitnesses[idx]\n",
    "    \n",
    "    return population[best_idx]\n",
    "\n",
    "print(\"Tournament selection defined (k=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4e23b",
   "metadata": {},
   "source": [
    "## Cell 9: Crossover Operator - One-Point Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Crossover Operator - One-Point Crossover\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    \"\"\"\n",
    "    One-Point Crossover for text.\n",
    "    \n",
    "    Split both parents at a random word position.\n",
    "    Combine Head(Parent1) + Tail(Parent2).\n",
    "    \n",
    "    DIFFERENCE FROM MATE:\n",
    "    - MATE uses Gemini to semantically blend styles\n",
    "    - Vanilla GA uses mechanical word-level concatenation\n",
    "    - Can create awkward, ungrammatical offspring\n",
    "    \n",
    "    Args:\n",
    "        parent1: First parent text\n",
    "        parent2: Second parent text\n",
    "    \n",
    "    Returns:\n",
    "        child: Offspring text\n",
    "    \"\"\"\n",
    "    words1 = parent1.split()\n",
    "    words2 = parent2.split()\n",
    "    \n",
    "    # Avoid splitting at the very beginning or end\n",
    "    if len(words1) < 3 or len(words2) < 3:\n",
    "        return parent1  # Too short for meaningful crossover\n",
    "    \n",
    "    # Random crossover point\n",
    "    crossover_point = random.randint(1, len(words1) - 1)\n",
    "    \n",
    "    # Create child: first part from parent1, second part from parent2\n",
    "    # Adjust second parent's split to maintain reasonable length\n",
    "    second_part_start = random.randint(0, max(0, len(words2) - (len(words1) - crossover_point)))\n",
    "    \n",
    "    child_words = words1[:crossover_point] + words2[second_part_start:]\n",
    "    child = ' '.join(child_words)\n",
    "    \n",
    "    return child\n",
    "\n",
    "# Test\n",
    "parent1 = \"The quick brown fox jumps over the lazy dog every morning.\"\n",
    "parent2 = \"A fast red cat leaps across the sleeping hound each evening.\"\n",
    "\n",
    "print(\"Crossover test:\")\n",
    "print(f\"  Parent 1: {parent1}\")\n",
    "print(f\"  Parent 2: {parent2}\")\n",
    "print(f\"\\nOffspring examples:\")\n",
    "for i in range(3):\n",
    "    child = one_point_crossover(parent1, parent2)\n",
    "    print(f\"  {i+1}. {child}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc66f3",
   "metadata": {},
   "source": [
    "## Cell 10: Mutation Operator - Uniform Random Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92291f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Mutation Operator - Uniform Random Mutation\n",
    "def uniform_mutation(text, mutation_rate=0.1):\n",
    "    \"\"\"\n",
    "    Uniform Random Mutation.\n",
    "    \n",
    "    For each word, with probability mutation_rate:\n",
    "    - Replace it with a random synonym\n",
    "    \n",
    "    CRITICAL DIFFERENCE FROM MATE:\n",
    "    - MATE uses saliency to identify WHICH words to mutate\n",
    "    - MATE uses Gemini to generate contextually appropriate replacements\n",
    "    - Vanilla GA blindly mutates random words\n",
    "    - No gradient information, no intelligence\n",
    "    \n",
    "    This is the KEY weakness of vanilla GA for this problem.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        mutation_rate: Probability of mutating each word (0.1 = 10%)\n",
    "    \n",
    "    Returns:\n",
    "        Mutated text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if random.random() < mutation_rate:\n",
    "            word = words[i]\n",
    "            # Remove punctuation for synonym lookup\n",
    "            clean_word = word.strip('.,!?;:\"\\'\"').lower()\n",
    "            \n",
    "            synonyms = get_synonyms(clean_word)\n",
    "            if synonyms:\n",
    "                # Randomly pick a synonym (no fitness check!)\n",
    "                new_word = random.choice(synonyms)\n",
    "                \n",
    "                # Preserve capitalization\n",
    "                if word and word[0].isupper():\n",
    "                    new_word = new_word.capitalize()\n",
    "                \n",
    "                # Preserve punctuation\n",
    "                for punct in '.,!?;:\\'\"':\n",
    "                    if word.endswith(punct):\n",
    "                        new_word += punct\n",
    "                        break\n",
    "                \n",
    "                words[i] = new_word\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Test\n",
    "test_text = \"The quick brown fox jumps over the lazy dog every morning.\"\n",
    "print(\"Mutation test (mutation_rate=0.3 for visibility):\")\n",
    "print(f\"  Original: {test_text}\")\n",
    "print(f\"\\nRandom mutations:\")\n",
    "for i in range(3):\n",
    "    mutated = uniform_mutation(test_text, mutation_rate=0.3)\n",
    "    print(f\"  {i+1}. {mutated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e53f0",
   "metadata": {},
   "source": [
    "## Cell 11: Logging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0889e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Logging Functions\n",
    "def initialize_csv():\n",
    "    \"\"\"Initialize CSV file with headers.\"\"\"\n",
    "    with open(GA_CSV, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            'generation', 'individual_id', 'fitness', 'p_human', 'p_ai', 'p_mimicry',\n",
    "            'semantic_similarity', 'penalty', 'predicted_class', 'is_best'\n",
    "        ])\n",
    "\n",
    "def log_to_csv(generation, population, fitnesses, original_text):\n",
    "    \"\"\"Log all individuals in a generation to CSV.\"\"\"\n",
    "    best_idx = np.argmax(fitnesses)\n",
    "    with open(GA_CSV, \"a\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for i, (text, fitness) in enumerate(zip(population, fitnesses)):\n",
    "            _, components = calculate_fitness(text, original_text)\n",
    "            probs, pred_class, _ = get_classifier_predictions(text)\n",
    "            writer.writerow([\n",
    "                generation, i+1, f\"{fitness:.8f}\",\n",
    "                f\"{probs[0]:.8f}\", f\"{probs[1]:.8f}\", f\"{probs[2]:.8f}\",\n",
    "                f\"{components['similarity']:.4f}\",\n",
    "                f\"{components['penalty']:.4f}\",\n",
    "                pred_class+1, 1 if i==best_idx else 0\n",
    "            ])\n",
    "\n",
    "def log_generation(generation, population, fitnesses, original_text):\n",
    "    \"\"\"Log generation details to text file.\"\"\"\n",
    "    filepath = GA_DIR / f\"generation_{generation}.txt\"\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"VANILLA GA - GENERATION {generation}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # Sort by fitness\n",
    "        sorted_indices = np.argsort(fitnesses)[::-1]\n",
    "        \n",
    "        for rank, idx in enumerate(sorted_indices[:5], 1):  # Top 5 only\n",
    "            text = population[idx]\n",
    "            fitness = fitnesses[idx]\n",
    "            \n",
    "            _, components = calculate_fitness(text, original_text)\n",
    "            probs, pred_class, conf = get_classifier_predictions(text)\n",
    "            \n",
    "            f.write(f\"Rank {rank} | Individual {idx + 1}\\n\\n\")\n",
    "            f.write(f\"Fitness: {fitness:.4f}\\n\")\n",
    "            f.write(f\"  - P(Human): {components['p_human']:.4f}\\n\")\n",
    "            f.write(f\"  - P(AI): {probs[1]:.4f}\\n\")\n",
    "            f.write(f\"  - P(AI-mimicry): {probs[2]:.4f}\\n\")\n",
    "            f.write(f\"  - Semantic similarity: {components['similarity']:.4f}\\n\")\n",
    "            f.write(f\"  - Penalty: {components['penalty']:.4f}\\n\")\n",
    "            f.write(f\"  - Predicted class: {pred_class+1} (conf={conf:.4f})\\n\")\n",
    "            f.write(f\"\\nText:\\n{text}\\n\")\n",
    "            f.write(f\"\\n{'=' * 80}\\n\\n\")\n",
    "        \n",
    "        # Summary\n",
    "        f.write(f\"SUMMARY STATISTICS\\n\")\n",
    "        f.write(f\"Best fitness: {max(fitnesses):.4f}\\n\")\n",
    "        f.write(f\"Avg fitness:  {np.mean(fitnesses):.4f}\\n\")\n",
    "        f.write(f\"Worst fitness: {min(fitnesses):.4f}\\n\")\n",
    "\n",
    "print(\"Logging functions defined\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
