# Task 4: Memetic Algorithm Text Evolution (MATE) ü¶ò

**I want to jailbreak your model**, one might say. Well, so do I. That's exactly what I'm going to do now here in task-4.  

We were told to use a *genetic* algorithm to jailbreak [the transformer model](../task-2/transformer/tier_c_final_model/). I decided instead, to use a memetic algorithm for the same.  
They're similar, but they're **not the same thing**...

I explain what memetic algorithms are in detail in [memetic.md](memetic.md).

Please look at [this dictionary](#dictionary)

To put it simply, **I am implementing a memetic algorithm instead of a genetic algorithm simply because of local search**. I believe this should be quite helpful in our specific usecase, because if the AI finds something good it can exploit as human, it can explore that fully and go on exploiting it.

**All RESULTS** as and when they were happening, as well as reports of my failure, are documented in detail in [results/](results/)


# The structure of the algorithm

## Step 1: Define the objective

We treat detector evasion as an optimization problem.

- **Input**: an AI-generated paragraph
- **Final Output**: a modified paragraph that the classifier labels as *human*
- **Fitness signal**: the detector‚Äôs ‚ÄúHuman‚Äù probability score

At the same time, we impose soft constraints:
- The meaning should stay close to the original
- The text should remain fluent and readable
- The paragraph should not be completely rewritten  

This is done so that it doesn't do something like *"oh, human written text has a higher hapax legomena, let me just put every single word in the dictionary so that the number of unique words maximises and thus I am always classified as human."*


## Step 2: Reduce the search space

Directly mutating every word is infeasible...

- Compute token-level saliency scores using the trained classifier
- Sleect only the top-impact tokens (for example, top 10‚Äì20%) and freeze all the other token

This significantly reduces the dimensionality of the search and makes evolution controllable.

## Step 3: Initialize the population

We generate an initial population of candidate paragraphs.

- Start with 10 AI-generated paragraphs
- Apply light, guided perturbations to the mutable tokens
- Ensure all candidates are valid, fluent text


## Step 4: Fitness function

Each candidate paragraph is evaluated using a single scalar fitness score:

- **Objective**: lower detector confidence for ‚ÄúAI‚Äù
- **Penalties**:
  - Semantic drift beyond a threshold
  - Excessive perplexity

I intend to use soft penalties so the algorithm can explore trade-offs and recover from bad intermediate steps.

This whole step is done through a process referred to as [**Lagrangian Relaxation**](lagrangian-relaxation.md).


## Step 5: Global evolution (genetic search)

We evolve the population over several generations.

Each generation consists of:
- **Selection**: keep the top-performing candidates
- **Crossover**: combine compatible token substitutions from parents
- **Mutation**: use an LLM to rewrite or perturb small parts of the text
  - Change sentence rhythm
  - Introduce subtle irregularities
  - Swap synonyms or phrasing
  - BUT remember not to change the paragraph entirely


## Step 6: Local search

For the best individuals in each generation, we apply local search.

- Identify the most influential remaining tokens
- Explore small, local alternatives (synonyms, paraphrases)
- Accept improvements greedily
- Occasionally accept worse moves with small probability to escape local optima (This is a process called **simulated annealing**).


## Step 7: Deterministic Crowding to preserve diversity

To avoid all solutions collapsing into the same pattern:
- Each child competes only with its most similar parent and the child replace the parent only if it is fitter
- Distinct stylistic strategies are preserved across generations


## Step 8: Iterate and observe

We repeat evolution. Every generation is printed to a new `.txt` file, so that we can visualise the step by step improvement. This is stored in the .txt file along with 

The goal is to see whether structured evolution can reliably push AI-written text past the detector‚Äôs decision boundary.


# Example: Evolving AI Text into Human Text

This example walks through a single run of MATE on a very small scale.  
The goal is to show *what actually changes* across generations.  
This example is generated by ChatGPT, prompted with my above 8-step iteration. I have cross-verified the correctness.

---

## Initial paragraph (source)

This paragraph is generated by an LLM and is clearly AI-like.

> *Technology has become an integral part of modern life, shaping the way individuals communicate, work, and perceive the world. Through innovation and advancement, society continues to evolve in unprecedented ways.*

Our detector assigns this paragraph:
- **AI probability**: 0.97
- **Human probability**: 0.03

---

## Generation 0: Initial population (very obviously AI)

We generate 5 candidate paragraphs.  
All are fluent, but clearly generic and polished.

### Population (G‚ÇÄ)

1. *Technology has become an integral part of modern life, influencing communication, productivity, and human interaction across the globe.*

2. *In today‚Äôs world, technology plays a vital role in shaping how people connect, innovate, and navigate daily experiences.*

3. *Modern society is deeply intertwined with technology, which continues to transform communication, labor, and cultural understanding.*

4. *The rapid advancement of technology has reshaped modern life, enabling new forms of interaction and unprecedented progress.*

5. *Technology remains a cornerstone of contemporary society, driving change in how individuals engage with information and one another.*

**Detector scores (Human probability):**
- [0.04, 0.06, 0.05, 0.07, 0.05]

We keep the **top 3**:
- (2), (4), (1)

---

## Saliency-based variable fixing

We compute token saliency using the detector.

High-impact tokens include:
- *integral*
- *modern*
- *shaping*
- *unprecedented*
- *society*

Low-impact tokens (articles, auxiliaries, prepositions) are frozen.

Only ~15% of tokens are mutable going forward.

---

## Generation 1: Mutation + crossover

We prompt the LLM to perturb the winners:

> ‚ÄúRewrite this paragraph to slightly disrupt sentence rhythm while keeping meaning.‚Äù

### Offspring (G‚ÇÅ)

1. *Technology has settled into everyday life, quietly influencing how people talk, work, and keep track of the world around them.*

2. *Technology now sits at the center of daily routines, changing how people communicate and get things done, often without noticing.*

3. *Modern life leans heavily on technology, shaping communication and work in ways that feel normal only because we‚Äôve grown used to them.*

**Detector scores (Human probability):**
- [0.22, 0.31, 0.28]

Notice:
- Less formal phrasing
- Fewer abstract nouns
- More concrete, conversational tone

---

## Generation 2: Memetic local search (refinement)

We apply local search to the best candidate (2).

Original:
> *Technology now sits at the center of daily routines, changing how people communicate and get things done, often without noticing.*

We explore local alternatives:
- Replace *‚Äúsits at the center‚Äù* ‚Üí *‚Äúhas worked its way into‚Äù*
- Replace *‚Äúget things done‚Äù* ‚Üí *‚Äúgo about their day‚Äù*

Refined version:
> *Technology has worked its way into daily routines, changing how people communicate and go about their day, often without much thought.*

**Detector score (Human probability):**
- **0.56**

---

## Generation 3: Controlled imperfection

Mutation prompt:
> ‚ÄúIntroduce a subtle irregularity or slightly informal phrasing.‚Äù

New candidate:
> *Technology has worked its way into daily routines, changing how people communicate and go about their day, often without giving it much thought at all.*

Detector:
- **Human probability**: 0.74

---

## Generation 4: Final polishing

One more local search step:
- Slight redundancy
- Mild conversational emphasis
- Still fluent, but less polished

Final paragraph:
> *Technology has worked its way into daily routines, changing how people communicate and go about their day, often without really giving it much thought.*

**Final detector score:**
- **Human probability**: **0.91**

---

## What changed (important)

This evolution did **not**:
- Add spelling mistakes
- Break grammar
- Inject obvious ‚Äúhuman tricks‚Äù

Instead, it:
- Reduced abstraction
- Softened sentence rhythm
- Added mild redundancy
- Removed overly clean phrasing

These are *stylistic signals*, not surface noise.

---

## Key takeaway

The detector was not fooled by topic or meaning.  
It was fooled by **how the sentence felt**.

This example illustrates why adversarial text evolution is best viewed as:
- A structured search problem
- With constraints
- And local refinement

Not as random corruption.




# Dictionary

## 1. Candidate
A single paragraph of text being evaluated and evolved.

## 2. Population
A fixed-size set of candidate paragraphs considered at the same time.

## 3. Generation
One iteration of evolution over the population.

## 4. P(Human)
Probability that a candidate paragraph is human-written.

## 5. Fitnesss in log space
The logarithm of P(Human), used to spread out very small probabilities and provide a usable optimization signal.

## 6. Semantic Similarity
A numerical measure of how closely a candidate preserves the meaning of the original paragraph.

## 7. Perplexity
A language-model-based measure of text fluency.

## 8. sem_pen / Semantic Penalty
A penalty applied when semantic similarity falls below a threshold.

## 9. ppl_pen / Perplexity Penalty
A penalty applied when perplexity exceeds a fluency threshold.

## 10. Lagrangian Relaxation
A method that incorporates constraints into the fitness function via penalties rather than hard rejection.

## 11. Fitness
A single scalar value combining attack success and penalties, used to compare candidates.  

fitness(z) = log(P(Human‚à£z)) ‚àí Œª<sub>sem</sub>(sem_pen(z)) ‚àí Œª<sub>ppl</sub>(ppl_pen(z))

## 12. Elitism
Direct copying of best individuals from the previous generation to the next

## 13. Lagrangian Relaxation
Basically, all Lagrangian relaxation does is covert a constrained problem into an unconstrained (or softly constrained) problem.  

e.g. `Minimise f(x) subject to g(x) ‚â§ 0` can be rewritten as `minimise f(x)+k.g(x)`  
Here, k>>0 is a penalty term. If `g(x)>0`, the term will become significantly bigger.

This is a simplified version of what lagrangian relaxation is.  
For any constrained function that we have, we add a penalty term.  In this file, I explain Lagrangian Relaxation very simply.

Basically, all Lagrangian relaxation does is covert a constrained problem into an unconstrained (or softly constrained) problem.  

e.g. `Minimise f(x) subject to g(x) ‚â§ 0` can be rewritten as `minimise f(x)+k.g(x)`  
Here, k>>0 is a penalty term. If `g(x)>0`, the term will become significantly bigger.

This is a simplified version of what lagrangian relaxation is.  
For any constrained function that we have, we add a penalty term.  