{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d87604",
   "metadata": {},
   "source": [
    "# Task-3\n",
    "\n",
    "Here, we want to know why the model thinks a text is AI generated.  \n",
    "\n",
    "We are required to use either [SHAP](https://shap.readthedocs.io/en/latest/) or [Captum](https://captum.ai/) for this task. I have decided to use SHAP because:\n",
    "1. It seems to have nicer visualisations\n",
    "2. Captum seems to be primarily for PyTorch.\n",
    "\n",
    "> highlight the words in an **\"Imposter\" paragraph** that most strongly signaled \"AI\" to your Tier C model.\n",
    "\n",
    "This is a bit of a problem. I do not have 3 such paragraphs as recommended. I only have 1 paragraph which was class-1, but mistaken to be class-3. \n",
    "\n",
    "I instead have decided to use SHAP to analyse the ones classified with the least confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d0967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1100.78it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning dataset for low-confidence samples...\n",
      "Scanning Class 0 (1960 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [02:04<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning Class 1 (988 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988/988 [01:05<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning Class 2 (973 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 973/973 [01:15<00:00, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS: LOWEST CONFIDENCE SAMPLES ---\n",
      "\n",
      "[Class 0] Lowest Confidence:\n",
      "  Sample 1 | Conf: 1.93% | File: 099_Sketches-New-and-Old,-Part-7._12.txt\n",
      "  Sample 2 | Conf: 46.85% | File: 013_Psmith-in-the-City_11.txt\n",
      "  Sample 3 | Conf: 61.68% | File: 043_Pericles-Prince-of-Tyre_14.txt\n",
      "\n",
      "[Class 1] Lowest Confidence:\n",
      "  Sample 1 | Conf: 80.86% | File: HLB_2_Bruce_Partington_Plans_T01_P01.txt\n",
      "  Sample 2 | Conf: 96.01% | File: The-Life-of-Henry-the-Eighth_T03_P01.txt\n",
      "  Sample 3 | Conf: 97.44% | File: The-Innocents-Abroad-—-Volume-04_T03_P02.txt\n",
      "\n",
      "[Class 2] Lowest Confidence:\n",
      "  Sample 1 | Conf: 5.75% | File: TWAIN_Roughing-It,-Part-5._T04_01.txt\n",
      "  Sample 2 | Conf: 12.54% | File: WODE_Love-Among-the-Chickens_T01_01.txt\n",
      "  Sample 3 | Conf: 32.15% | File: TWAIN_Mark-Twain's-Letters-—-Complete_T03_02.txt\n",
      "\n",
      "Scanning complete! Results saved to 'low_confidence_analysis'\n",
      "Lowest confidence samples saved to 'low_confidence_analysis/lowest_samples.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_PATH = \"../task-2/transformer/tier_c_final_model\"\n",
    "DATASET_DIR = Path('../dataset')\n",
    "OUTPUT_DIR = \"low_confidence_analysis\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(\"Loading model for inference...\")\n",
    "config = PeftConfig.from_pretrained(MODEL_PATH)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, num_labels=3\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "def load_and_scan():\n",
    "    results = {0: [], 1: [], 2: []}\n",
    "    \n",
    "    paths = {\n",
    "        0: (DATASET_DIR / 'class1-human-written', ['01-arthur-conan-doyle', '02-pg-wodehouse', '03-mark-twain', '04-william-shakespeare'], 'extracted_paragraphs'),\n",
    "        1: (DATASET_DIR / 'class2-ai-written', ['ai-generated-paragraphs'], ''), \n",
    "        2: (DATASET_DIR / 'class3-ai-mimicry', ['01-arthur-conan-doyle', '02-pg-wodehouse', '03-mark-twain', '04-william-shakespeare'], '')\n",
    "    }\n",
    "\n",
    "    print(\"\\nScanning dataset for low-confidence samples...\")\n",
    "    \n",
    "    for label, (base_path, subfolders, suffix) in paths.items():\n",
    "        files = []\n",
    "        for sub in subfolders:\n",
    "            if suffix:\n",
    "                search_path = base_path / sub / suffix\n",
    "            else:\n",
    "                if sub == 'ai-generated-paragraphs': \n",
    "                    search_path = base_path / sub\n",
    "                else:\n",
    "                    search_path = base_path / sub\n",
    "            \n",
    "            files.extend(glob.glob(os.path.join(str(search_path), '*.txt')))\n",
    "\n",
    "        print(f\"Scanning Class {label} ({len(files)} files)...\")\n",
    "        \n",
    "        for file_path in tqdm(files):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().strip()\n",
    "                \n",
    "                if not text: continue\n",
    "\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "                with torch.no_grad():\n",
    "                    logits = model(**inputs).logits\n",
    "                    probs = torch.softmax(logits, dim=1)[0]\n",
    "                \n",
    "                confidence = probs[label].item()\n",
    "                \n",
    "                results[label].append({\n",
    "                    'confidence': confidence,\n",
    "                    'text': text,\n",
    "                    'file': os.path.basename(file_path),\n",
    "                    'probs': probs.cpu().numpy()\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return results\n",
    "\n",
    "data_map = load_and_scan()\n",
    "lowest_samples = {}\n",
    "\n",
    "print(\"\\n--- RESULTS: LOWEST CONFIDENCE SAMPLES ---\")\n",
    "\n",
    "file_names = {\n",
    "    0: \"low-confidence-class-1-human.txt\",\n",
    "    1: \"low-confidence-class-2-generic.txt\",\n",
    "    2: \"low-confidence-class-3-mimic.txt\"\n",
    "}\n",
    "\n",
    "for label in [0, 1, 2]:\n",
    "    sorted_samples = sorted(data_map[label], key=lambda x: x['confidence'])\n",
    "    \n",
    "    bottom_3 = sorted_samples[:3]\n",
    "    lowest_samples[label] = bottom_3\n",
    "    \n",
    "    out_file = os.path.join(OUTPUT_DIR, file_names[label])\n",
    "    with open(out_file, 'w', encoding='utf-8') as f:\n",
    "        print(f\"\\n[Class {label}] Lowest Confidence:\")\n",
    "        for i, sample in enumerate(bottom_3):\n",
    "            header = f\"Sample {i+1} | Conf: {sample['confidence']:.2%} | File: {sample['file']}\"\n",
    "            print(f\"  {header}\")\n",
    "            f.write(f\"{header}\\n\")\n",
    "            f.write(f\"{sample['text']}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# Save lowest_samples for later SHAP analysis\n",
    "samples_file = os.path.join(OUTPUT_DIR, \"lowest_samples.pkl\")\n",
    "with open(samples_file, 'wb') as f:\n",
    "    pickle.dump(lowest_samples, f)\n",
    "\n",
    "print(f\"\\nScanning complete! Results saved to '{OUTPUT_DIR}'\")\n",
    "print(f\"Lowest confidence samples saved to '{samples_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91be408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/Documents/precog-task/human-or-ai/venv3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for SHAP analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1048.36it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running SHAP Analysis on the 9 lowest confidence samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PartitionExplainer explainer: 10it [03:52, 25.88s/it]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING 3 VISUALIZATIONS PER SAMPLE\n",
      "Processing Class_1_Human_Sample_1...\n",
      "Processing Class_1_Human_Sample_2...\n",
      "Processing Class_1_Human_Sample_3...\n",
      "Processing Class_2_Generic_AI_Sample_1...\n",
      "Processing Class_2_Generic_AI_Sample_2...\n",
      "Processing Class_2_Generic_AI_Sample_3...\n",
      "Processing Class_3_Mimic_AI_Sample_1...\n",
      "Processing Class_3_Mimic_AI_Sample_2...\n",
      "Processing Class_3_Mimic_AI_Sample_3...\n",
      "\n",
      "ANALYSIS COMPLETE!\n",
      "Check the 'low_confidence_analysis' folder for your .png and .html files.\n",
      "\n",
      "The visualizations now show:\n",
      "  • Bar charts: Actual words/tokens ranked by importance\n",
      "  • Heatmaps: Full text with color-coded word importance\n",
      "  • Red highlights = words that INCREASE the prediction for that class\n",
      "  • Blue highlights = words that DECREASE the prediction for that class\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "MODEL_PATH = \"../task-2/transformer/tier_c_final_model\"\n",
    "OUTPUT_DIR = \"low_confidence_analysis\"\n",
    "\n",
    "# Make sure output directory exists for the images\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Load the saved lowest confidence samples\n",
    "samples_file = os.path.join(OUTPUT_DIR, \"lowest_samples.pkl\")\n",
    "with open(samples_file, 'rb') as f:\n",
    "    lowest_samples = pickle.load(f) # Expected format: {0: [...], 1: [...], 2: [...]}\n",
    "\n",
    "# 2. MODEL LOADING\n",
    "print(\"Loading model for SHAP analysis...\")\n",
    "config = PeftConfig.from_pretrained(MODEL_PATH)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, num_labels=3\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 3. PREDICTION FUNCTION\n",
    "def predict_shap(texts):\n",
    "    # Handle single string vs list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    elif isinstance(texts, (list, np.ndarray)):\n",
    "        texts = [str(t) for t in texts]\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        texts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        scores = torch.softmax(logits, dim=1)\n",
    "        \n",
    "    return scores.cpu().numpy()\n",
    "\n",
    "# 4. HELPER FUNCTION TO GET ACTUAL TOKENS\n",
    "def get_token_text(text, token_id_or_index):\n",
    "    \"\"\"Convert token ID or index to actual text representation\"\"\"\n",
    "    try:\n",
    "        # If it's an integer, decode it directly\n",
    "        if isinstance(token_id_or_index, (int, np.integer)):\n",
    "            return tokenizer.decode([token_id_or_index])\n",
    "        # If it's already a string token, return as is\n",
    "        return str(token_id_or_index)\n",
    "    except:\n",
    "        return f\"[UNK_{token_id_or_index}]\"\n",
    "\n",
    "# 5. RUN SHAP\n",
    "print(\"\\nRunning SHAP Analysis on the 9 lowest confidence samples...\")\n",
    "explainer = shap.Explainer(predict_shap, tokenizer)\n",
    "\n",
    "texts_to_explain = []\n",
    "target_classes = [] # We need to know which class to visualize for each sample\n",
    "sample_names = []\n",
    "\n",
    "# Flatten the dictionary into lists\n",
    "for label in [0, 1, 2]:\n",
    "    class_name = [\"Human\", \"Generic_AI\", \"Mimic_AI\"][label]\n",
    "    for idx, sample in enumerate(lowest_samples[label]):\n",
    "        texts_to_explain.append(sample['text'])\n",
    "        # We want to visualize the TRUE class to see why confidence was low\n",
    "        target_classes.append(label) \n",
    "        sample_names.append(f\"Class_{label+1}_{class_name}_Sample_{idx+1}\")\n",
    "\n",
    "# Calculate SHAP values (might take a minute)\n",
    "shap_values = explainer(texts_to_explain)\n",
    "\n",
    "# 6. GENERATE VISUALIZATIONS\n",
    "print(\"GENERATING 3 VISUALIZATIONS PER SAMPLE\")\n",
    "\n",
    "for i in range(len(texts_to_explain)):\n",
    "    name = sample_names[i]\n",
    "    target_cls = target_classes[i]\n",
    "    print(f\"Processing {name}...\")\n",
    "    \n",
    "    # Get the actual text for this sample to properly map tokens\n",
    "    current_text = texts_to_explain[i]\n",
    "    \n",
    "    # Tokenize to get the actual token IDs and convert them to text\n",
    "    encoded = tokenizer(current_text, truncation=True, max_length=512)\n",
    "    token_ids = encoded['input_ids']\n",
    "    actual_tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "\n",
    "    # VISUALIZATION 1: WATERFALL PLOT (The Logic)\n",
    "    # Shows the Top ~12 words that pushed the score up or down\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.plots.waterfall(\n",
    "            shap_values[i, :, target_cls], \n",
    "            max_display=12, \n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f\"{name}: Why class {target_cls+1}?\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{name}_waterfall.png\"))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Waterfall plot failed for {name}: {e}\")\n",
    "\n",
    "    # VISUALIZATION 2: BAR CHART (The Summary)\n",
    "    # Shows global importance of features for this sample\n",
    "    try:\n",
    "        # Get SHAP values for this sample and target class\n",
    "        shap_vals_single = shap_values[i, :, target_cls].values\n",
    "        \n",
    "        # Get feature names - these should be the actual tokens\n",
    "        if hasattr(shap_values[i, :, target_cls], 'data') and shap_values[i, :, target_cls].data is not None:\n",
    "            feature_names = shap_values[i, :, target_cls].data\n",
    "        else:\n",
    "            # Fallback to using actual_tokens we computed\n",
    "            feature_names = actual_tokens[:len(shap_vals_single)]\n",
    "        \n",
    "        # Ensure we have the right number of features\n",
    "        if len(feature_names) != len(shap_vals_single):\n",
    "            feature_names = actual_tokens[:len(shap_vals_single)]\n",
    "        \n",
    "        # Get top features by absolute value\n",
    "        abs_vals = np.abs(shap_vals_single)\n",
    "        top_indices = np.argsort(abs_vals)[-12:][::-1]\n",
    "        \n",
    "        top_vals = shap_vals_single[top_indices]\n",
    "        top_features = []\n",
    "        for idx in top_indices:\n",
    "            if idx < len(feature_names):\n",
    "                token = str(feature_names[idx]).strip()\n",
    "                # Clean up token representation\n",
    "                if token.startswith('##'):\n",
    "                    token = token[2:]  # Remove BERT subword prefix\n",
    "                if not token or token == '':\n",
    "                    token = '[SPACE]'\n",
    "                top_features.append(token)\n",
    "            else:\n",
    "                top_features.append('[UNK]')\n",
    "        \n",
    "        # Create the bar plot\n",
    "        colors = ['#ff0051' if v > 0 else '#008bfb' for v in top_vals]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(top_vals)), top_vals, color=colors)\n",
    "        plt.yticks(range(len(top_vals)), top_features)\n",
    "        plt.xlabel('SHAP value (impact on model output)')\n",
    "        plt.title(f\"{name}: Top Features (Words)\", fontsize=14)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{name}_bar.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error creating bar plot for {name}: {e}\")\n",
    "\n",
    "    # VISUALIZATION 3: TEXT HEATMAP (The Context)\n",
    "    # Shows the text with Red/Blue highlights\n",
    "    try:\n",
    "        # Get SHAP values for this sample\n",
    "        shap_vals_single = shap_values[i, :, target_cls].values\n",
    "        \n",
    "        # Use actual_tokens we computed earlier\n",
    "        tokens_for_viz = actual_tokens[:len(shap_vals_single)]\n",
    "        \n",
    "        # Ensure alignment\n",
    "        if len(tokens_for_viz) != len(shap_vals_single):\n",
    "            print(f\"  Warning: Token count mismatch for {name}\")\n",
    "            tokens_for_viz = tokens_for_viz[:len(shap_vals_single)]\n",
    "        \n",
    "        max_abs = max(abs(shap_vals_single)) if len(shap_vals_single) > 0 else 1\n",
    "        \n",
    "        html = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{name}</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            margin: 20px;\n",
    "            font-family: Arial, sans-serif;\n",
    "            max-width: 1200px;\n",
    "        }}\n",
    "        h1 {{\n",
    "            color: #333;\n",
    "            border-bottom: 2px solid #ccc;\n",
    "            padding-bottom: 10px;\n",
    "        }}\n",
    "        .legend {{\n",
    "            margin: 20px 0;\n",
    "            padding: 10px;\n",
    "            background: #f5f5f5;\n",
    "            border-radius: 5px;\n",
    "        }}\n",
    "        .text-container {{\n",
    "            font-family: 'Courier New', monospace;\n",
    "            line-height: 2;\n",
    "            font-size: 14px;\n",
    "            padding: 20px;\n",
    "            background: white;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 5px;\n",
    "        }}\n",
    "        .token {{\n",
    "            padding: 2px 4px;\n",
    "            margin: 1px;\n",
    "            border-radius: 3px;\n",
    "            display: inline-block;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>{name.replace('_', ' ')}</h1>\n",
    "    <div class=\"legend\">\n",
    "        <strong>Legend:</strong> \n",
    "        <span style=\"background-color: rgba(255, 0, 81, 0.5); padding: 2px 8px; border-radius: 3px;\">Red = Increases prediction</span>\n",
    "        <span style=\"background-color: rgba(0, 139, 251, 0.5); padding: 2px 8px; border-radius: 3px; margin-left: 10px;\">Blue = Decreases prediction</span>\n",
    "    </div>\n",
    "    <div class=\"text-container\">\"\"\"\n",
    "        \n",
    "        for token, val in zip(tokens_for_viz, shap_vals_single):\n",
    "            # Clean up token\n",
    "            token_text = str(token).strip()\n",
    "            if token_text.startswith('##'):\n",
    "                token_text = token_text[2:]  # Remove BERT subword prefix\n",
    "                space = ''  # No space before subword\n",
    "            elif token_text in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue  # Skip special tokens\n",
    "            else:\n",
    "                space = ' '  # Add space before regular tokens\n",
    "            \n",
    "            if not token_text:\n",
    "                token_text = '·'  # Use middle dot for empty tokens\n",
    "            \n",
    "            # Color based on SHAP value\n",
    "            intensity = min(abs(val) / max_abs, 1.0) * 0.7 if max_abs > 0 else 0\n",
    "            \n",
    "            if val > 0:\n",
    "                color = f\"rgba(255, 0, 81, {intensity})\"\n",
    "            else:\n",
    "                color = f\"rgba(0, 139, 251, {intensity})\"\n",
    "            \n",
    "            html += f\"{space}<span class='token' style='background-color: {color};' title='SHAP: {val:.4f}'>{token_text}</span>\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "    </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        with open(os.path.join(OUTPUT_DIR, f\"{name}_heatmap.html\"), \"w\", encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error creating heatmap for {name}: {e}\")\n",
    "\n",
    "print(\"\\nANALYSIS COMPLETE!\")\n",
    "print(f\"Check the '{OUTPUT_DIR}' folder for your .png and .html files.\")\n",
    "print(\"\\nThe visualizations now show:\")\n",
    "print(\"  • Bar charts: Actual words/tokens ranked by importance\")\n",
    "print(\"  • Heatmaps: Full text with color-coded word importance\")\n",
    "print(\"  • Red highlights = words that INCREASE the prediction for that class\")\n",
    "print(\"  • Blue highlights = words that DECREASE the prediction for that class\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
