{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d87604",
   "metadata": {},
   "source": [
    "# Task-3\n",
    "\n",
    "Here, we want to know why the model thinks a text is AI generated.  \n",
    "\n",
    "We are required to use either [SHAP](https://shap.readthedocs.io/en/latest/) or [Captum](https://captum.ai/) for this task. I have decided to use SHAP because:\n",
    "1. It seems to have nicer visualisations\n",
    "2. Captum seems to be primarily for PyTorch.\n",
    "\n",
    "> highlight the words in an **\"Imposter\" paragraph** that most strongly signaled \"AI\" to your Tier C model.\n",
    "\n",
    "This is a bit of a problem. I do not have 3 such paragraphs as recommended. I only have 1 paragraph which was class-1, but mistaken to be class-3. \n",
    "\n",
    "I instead have decided to use SHAP to analyse the ones classified with the least confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d0967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1100.78it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning dataset for low-confidence samples...\n",
      "Scanning Class 0 (1960 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [02:04<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning Class 1 (988 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988/988 [01:05<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning Class 2 (973 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 973/973 [01:15<00:00, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS: LOWEST CONFIDENCE SAMPLES ---\n",
      "\n",
      "[Class 0] Lowest Confidence:\n",
      "  Sample 1 | Conf: 1.93% | File: 099_Sketches-New-and-Old,-Part-7._12.txt\n",
      "  Sample 2 | Conf: 46.85% | File: 013_Psmith-in-the-City_11.txt\n",
      "  Sample 3 | Conf: 61.68% | File: 043_Pericles-Prince-of-Tyre_14.txt\n",
      "\n",
      "[Class 1] Lowest Confidence:\n",
      "  Sample 1 | Conf: 80.86% | File: HLB_2_Bruce_Partington_Plans_T01_P01.txt\n",
      "  Sample 2 | Conf: 96.01% | File: The-Life-of-Henry-the-Eighth_T03_P01.txt\n",
      "  Sample 3 | Conf: 97.44% | File: The-Innocents-Abroad-—-Volume-04_T03_P02.txt\n",
      "\n",
      "[Class 2] Lowest Confidence:\n",
      "  Sample 1 | Conf: 5.75% | File: TWAIN_Roughing-It,-Part-5._T04_01.txt\n",
      "  Sample 2 | Conf: 12.54% | File: WODE_Love-Among-the-Chickens_T01_01.txt\n",
      "  Sample 3 | Conf: 32.15% | File: TWAIN_Mark-Twain's-Letters-—-Complete_T03_02.txt\n",
      "\n",
      "Scanning complete! Results saved to 'low_confidence_analysis'\n",
      "Lowest confidence samples saved to 'low_confidence_analysis/lowest_samples.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_PATH = \"../task-2/transformer/tier_c_final_model\"\n",
    "DATASET_DIR = Path('../dataset')\n",
    "OUTPUT_DIR = \"low_confidence_analysis\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(\"Loading model for inference...\")\n",
    "config = PeftConfig.from_pretrained(MODEL_PATH)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, num_labels=3\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "def load_and_scan():\n",
    "    results = {0: [], 1: [], 2: []}\n",
    "    \n",
    "    paths = {\n",
    "        0: (DATASET_DIR / 'class1-human-written', ['01-arthur-conan-doyle', '02-pg-wodehouse', '03-mark-twain', '04-william-shakespeare'], 'extracted_paragraphs'),\n",
    "        1: (DATASET_DIR / 'class2-ai-written', ['ai-generated-paragraphs'], ''), \n",
    "        2: (DATASET_DIR / 'class3-ai-mimicry', ['01-arthur-conan-doyle', '02-pg-wodehouse', '03-mark-twain', '04-william-shakespeare'], '')\n",
    "    }\n",
    "\n",
    "    print(\"\\nScanning dataset for low-confidence samples...\")\n",
    "    \n",
    "    for label, (base_path, subfolders, suffix) in paths.items():\n",
    "        files = []\n",
    "        for sub in subfolders:\n",
    "            if suffix:\n",
    "                search_path = base_path / sub / suffix\n",
    "            else:\n",
    "                if sub == 'ai-generated-paragraphs': \n",
    "                    search_path = base_path / sub\n",
    "                else:\n",
    "                    search_path = base_path / sub\n",
    "            \n",
    "            files.extend(glob.glob(os.path.join(str(search_path), '*.txt')))\n",
    "\n",
    "        print(f\"Scanning Class {label} ({len(files)} files)...\")\n",
    "        \n",
    "        for file_path in tqdm(files):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().strip()\n",
    "                \n",
    "                if not text: continue\n",
    "\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "                with torch.no_grad():\n",
    "                    logits = model(**inputs).logits\n",
    "                    probs = torch.softmax(logits, dim=1)[0]\n",
    "                \n",
    "                confidence = probs[label].item()\n",
    "                \n",
    "                results[label].append({\n",
    "                    'confidence': confidence,\n",
    "                    'text': text,\n",
    "                    'file': os.path.basename(file_path),\n",
    "                    'probs': probs.cpu().numpy()\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return results\n",
    "\n",
    "data_map = load_and_scan()\n",
    "lowest_samples = {}\n",
    "\n",
    "print(\"\\n--- RESULTS: LOWEST CONFIDENCE SAMPLES ---\")\n",
    "\n",
    "file_names = {\n",
    "    0: \"low-confidence-class-1-human.txt\",\n",
    "    1: \"low-confidence-class-2-generic.txt\",\n",
    "    2: \"low-confidence-class-3-mimic.txt\"\n",
    "}\n",
    "\n",
    "for label in [0, 1, 2]:\n",
    "    sorted_samples = sorted(data_map[label], key=lambda x: x['confidence'])\n",
    "    \n",
    "    bottom_3 = sorted_samples[:3]\n",
    "    lowest_samples[label] = bottom_3\n",
    "    \n",
    "    out_file = os.path.join(OUTPUT_DIR, file_names[label])\n",
    "    with open(out_file, 'w', encoding='utf-8') as f:\n",
    "        print(f\"\\n[Class {label}] Lowest Confidence:\")\n",
    "        for i, sample in enumerate(bottom_3):\n",
    "            header = f\"Sample {i+1} | Conf: {sample['confidence']:.2%} | File: {sample['file']}\"\n",
    "            print(f\"  {header}\")\n",
    "            f.write(f\"{header}\\n\")\n",
    "            f.write(f\"{sample['text']}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# Save lowest_samples for later SHAP analysis\n",
    "samples_file = os.path.join(OUTPUT_DIR, \"lowest_samples.pkl\")\n",
    "with open(samples_file, 'wb') as f:\n",
    "    pickle.dump(lowest_samples, f)\n",
    "\n",
    "print(f\"\\nScanning complete! Results saved to '{OUTPUT_DIR}'\")\n",
    "print(f\"Lowest confidence samples saved to '{samples_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91be408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/Documents/precog-task/human-or-ai/venv3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for SHAP analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1097.45it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running SHAP Analysis on the 9 lowest confidence samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PartitionExplainer explainer: 10it [03:35, 23.95s/it]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING 3 VISUALIZATIONS PER SAMPLE\n",
      "Processing Class_1_Human_Sample_1...\n",
      "  Using custom bar plot for Class_1_Human_Sample_1 (SHAP error: list index out of range)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7049/1869846322.py:143: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Class_1_Human_Sample_2...\n",
      "  Using custom bar plot for Class_1_Human_Sample_2 (SHAP error: list index out of range)\n",
      "Processing Class_1_Human_Sample_3...\n",
      "  Using custom bar plot for Class_1_Human_Sample_3 (SHAP error: list index out of range)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7049/1869846322.py:143: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Class_2_Generic_AI_Sample_1...\n",
      "  Using custom bar plot for Class_2_Generic_AI_Sample_1 (SHAP error: list index out of range)\n",
      "Processing Class_2_Generic_AI_Sample_2...\n",
      "  Using custom bar plot for Class_2_Generic_AI_Sample_2 (SHAP error: list index out of range)\n",
      "Processing Class_2_Generic_AI_Sample_3...\n",
      "  Using custom bar plot for Class_2_Generic_AI_Sample_3 (SHAP error: list index out of range)\n",
      "Processing Class_3_Mimic_AI_Sample_1...\n",
      "  Using custom bar plot for Class_3_Mimic_AI_Sample_1 (SHAP error: list index out of range)\n",
      "Processing Class_3_Mimic_AI_Sample_2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7049/1869846322.py:143: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using custom bar plot for Class_3_Mimic_AI_Sample_2 (SHAP error: can only concatenate str (not \"list\") to str)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7049/1869846322.py:143: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Class_3_Mimic_AI_Sample_3...\n",
      "  Using custom bar plot for Class_3_Mimic_AI_Sample_3 (SHAP error: list index out of range)\n",
      "\n",
      "ANALYSIS COMPLETE!\n",
      "Check the 'low_confidence_analysis' folder for your .png and .html files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "MODEL_PATH = \"../task-2/transformer/tier_c_final_model\"\n",
    "OUTPUT_DIR = \"low_confidence_analysis\"\n",
    "\n",
    "# Make sure output directory exists for the images\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Load the saved lowest confidence samples\n",
    "samples_file = os.path.join(OUTPUT_DIR, \"lowest_samples.pkl\")\n",
    "with open(samples_file, 'rb') as f:\n",
    "    lowest_samples = pickle.load(f) # Expected format: {0: [...], 1: [...], 2: [...]}\n",
    "\n",
    "# 2. MODEL LOADING\n",
    "print(\"Loading model for SHAP analysis...\")\n",
    "config = PeftConfig.from_pretrained(MODEL_PATH)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, num_labels=3\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 3. PREDICTION FUNCTION\n",
    "def predict_shap(texts):\n",
    "    # Handle single string vs list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    elif isinstance(texts, (list, np.ndarray)):\n",
    "        texts = [str(t) for t in texts]\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        texts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        scores = torch.softmax(logits, dim=1)\n",
    "        \n",
    "    return scores.cpu().numpy()\n",
    "\n",
    "# 4. RUN SHAP\n",
    "print(\"\\nRunning SHAP Analysis on the 9 lowest confidence samples...\")\n",
    "explainer = shap.Explainer(predict_shap, tokenizer)\n",
    "\n",
    "texts_to_explain = []\n",
    "target_classes = [] # We need to know which class to visualize for each sample\n",
    "sample_names = []\n",
    "\n",
    "# Flatten the dictionary into lists\n",
    "for label in [0, 1, 2]:\n",
    "    class_name = [\"Human\", \"Generic_AI\", \"Mimic_AI\"][label]\n",
    "    for idx, sample in enumerate(lowest_samples[label]):\n",
    "        texts_to_explain.append(sample['text'])\n",
    "        # We want to visualize the TRUE class to see why confidence was low\n",
    "        target_classes.append(label) \n",
    "        sample_names.append(f\"Class_{label+1}_{class_name}_Sample_{idx+1}\")\n",
    "\n",
    "# Calculate SHAP values (might take a minute)\n",
    "shap_values = explainer(texts_to_explain)\n",
    "\n",
    "# 5. GENERATE VISUALIZATIONS\n",
    "print(\"GENERATING 3 VISUALIZATIONS PER SAMPLE\")\n",
    "\n",
    "for i in range(len(texts_to_explain)):\n",
    "    name = sample_names[i]\n",
    "    target_cls = target_classes[i]\n",
    "    print(f\"Processing {name}...\")\n",
    "\n",
    "    # VISUALIZATION 1: WATERFALL PLOT (The Logic)\n",
    "    # Shows the Top ~12 words that pushed the score up or down\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(\n",
    "        shap_values[i, :, target_cls], \n",
    "        max_display=12, \n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"{name}: Why class {target_cls+1}?\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f\"{name}_waterfall.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # VISUALIZATION 2: BAR CHART (The Summary)\n",
    "    # Shows global importance of features for this sample\n",
    "    # Fix: Create proper Explanation object with feature names\n",
    "    try:\n",
    "        single_sample_exp = shap.Explanation(\n",
    "            values=shap_values[i, :, target_cls].values,\n",
    "            base_values=shap_values[i, :, target_cls].base_values,\n",
    "            data=shap_values[i, :, target_cls].data,\n",
    "            feature_names=shap_values.feature_names\n",
    "        )\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.plots.bar(\n",
    "            single_sample_exp, \n",
    "            max_display=12,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f\"{name}: Top Features\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{name}_bar.png\"))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Using custom bar plot for {name} (SHAP error: {e})\")\n",
    "        \n",
    "        # Fallback: Create a custom bar plot from SHAP values\n",
    "        shap_vals = shap_values[i, :, target_cls].values\n",
    "        feature_names = shap_values.feature_names\n",
    "        \n",
    "        # Get top features by absolute value\n",
    "        abs_vals = np.abs(shap_vals)\n",
    "        top_indices = np.argsort(abs_vals)[-12:][::-1]\n",
    "        \n",
    "        top_vals = shap_vals[top_indices]\n",
    "        top_features = [feature_names[idx] if idx < len(feature_names) else f\"Token_{idx}\" \n",
    "                       for idx in top_indices]\n",
    "        \n",
    "        # Create the bar plot\n",
    "        colors = ['#ff0051' if v > 0 else '#008bfb' for v in top_vals]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(top_vals)), top_vals, color=colors)\n",
    "        plt.yticks(range(len(top_vals)), top_features)\n",
    "        plt.xlabel('SHAP value (impact on model output)')\n",
    "        plt.title(f\"{name}: Top Features\", fontsize=14)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{name}_bar.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # VISUALIZATION 3: TEXT HEATMAP (The Context)\n",
    "    # Shows the text with Red/Blue highlights\n",
    "    # We save this as HTML because it is interactive\n",
    "    try:\n",
    "        # SHAP returns raw SVG, so we wrap it in proper HTML\n",
    "        svg_content = shap.plots.text(\n",
    "            shap_values[i, :, target_cls], \n",
    "            display=False\n",
    "        )\n",
    "        \n",
    "        # Wrap the SVG in proper HTML structure\n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{name}</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            margin: 20px;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }}\n",
    "        h1 {{\n",
    "            color: #333;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>{name.replace('_', ' ')}</h1>\n",
    "    {svg_content}\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        with open(os.path.join(OUTPUT_DIR, f\"{name}_heatmap.html\"), \"w\", encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "    except Exception as e:\n",
    "        print(f\"  Using custom heatmap for {name} (SHAP error: {e})\")\n",
    "        \n",
    "        # Create a simple HTML fallback\n",
    "        shap_vals = shap_values[i, :, target_cls].values\n",
    "        tokens = shap_values.feature_names\n",
    "        max_abs = max(abs(shap_vals)) if len(shap_vals) > 0 else 1\n",
    "        \n",
    "        html = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{name}</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            margin: 20px;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }}\n",
    "        h1 {{\n",
    "            color: #333;\n",
    "        }}\n",
    "        .text-container {{\n",
    "            font-family: monospace;\n",
    "            line-height: 2;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>{name.replace('_', ' ')}</h1>\n",
    "    <div class=\"text-container\">\"\"\"\n",
    "        \n",
    "        for token, val in zip(tokens, shap_vals):\n",
    "            # Color based on SHAP value\n",
    "            intensity = min(abs(val) / max_abs, 1.0) * 0.7 if max_abs > 0 else 0\n",
    "            \n",
    "            if val > 0:\n",
    "                color = f\"rgba(255, 0, 81, {intensity})\"\n",
    "            else:\n",
    "                color = f\"rgba(0, 139, 251, {intensity})\"\n",
    "            \n",
    "            html += f\"<span style='background-color: {color}; padding: 2px;'>{token}</span> \"\n",
    "        \n",
    "        html += \"\"\"\n",
    "    </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        with open(os.path.join(OUTPUT_DIR, f\"{name}_heatmap.html\"), \"w\", encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "\n",
    "print(\"\\nANALYSIS COMPLETE!\")\n",
    "print(f\"Check the '{OUTPUT_DIR}' folder for your .png and .html files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
