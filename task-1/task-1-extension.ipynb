{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49b2567",
   "metadata": {},
   "source": [
    "This is an extension to the main task-1. It includes how information theoretic concepts (specifically perplexity) and geometric concepts (local intrinsic dimension) can be used to differentiate between AI and human-written text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3e87b",
   "metadata": {},
   "source": [
    "# V. Information-Theoretic Signatures\n",
    "\n",
    "Information theory quantifies predictability. AI, is very predictable. Or it should be. That's what we're going to see here...  \n",
    "\n",
    "Before getting started on the specifics, [this blog](https://kuiper2000.github.io/chaos_and_predictability/week9/week9) explains the basics of information theory, and how it ties into predictability quite well. I enjoyed reading it.\n",
    "\n",
    "I also want to preface by saying that I am by no means an expert / know much about information theory. I tried to learn a bit for the purposes of this task, but my domain knowledge is limited to that. I also thank the Infosec class and TA's for hinting the basics of this to us, which led me down this rabbit hole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e80f1f",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "[This blog](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584) explains perplexity really nicely through a simple example.\n",
    "\n",
    "In general, it is a measure of how well a given sentence is predicted. Or, in the sense of the word itself, how **perplexed** a model is when reading text. Lower perplexity => less perplexed, higher perplexity => more perplexed.\n",
    "\n",
    "- Because AI generates text by selecting high-probability tokens, the resulting text has statistically low perplexity. It follows the \"path of least resistance.\" However, since our temperature is relatively high (set to 1), the perplexity may be higher than usual AI generated text.\n",
    "- Human writing - especially famous authors - is replete with choices. Creative metaphors, sudden topic shifts, and idiomatic expressions that statistically defy the model's predictive expectations.\n",
    "\n",
    "\n",
    "### The maths behind perplexity\n",
    "\n",
    "This section is inspired by [Fabio Chiusano's Medium post.](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584)\n",
    "\n",
    "A language model predicts text **one word at a time**.\n",
    "\n",
    "For a sentence like:\n",
    "\n",
    "> **\"a red fox.\"**\n",
    "\n",
    "the model assigns probabilities like:\n",
    "\n",
    "- P(\"a\")\n",
    "- P(\"red\" | \"a\")\n",
    "- P(\"fox\" | \"a red\")\n",
    "- P(\".\" | \"a red fox\")\n",
    "\n",
    "To get the probability of the **whole sentence**, we multiply:\n",
    "\n",
    "$$\n",
    "P(W) = P(w_1) \\times P(w_2|w_1) \\times \\dots \\times P(w_n|w_1,\\dots,w_{n-1})\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "P(\\text{\"a red fox.\"}) = 0.4 \\times 0.27 \\times 0.55 \\times 0.79 = 0.0469\n",
    "$$\n",
    "\n",
    "**Problem: longer sentences always get smaller probabilities**\n",
    "\n",
    "Multiplying many numbers **smaller than 1** makes the result very small.\n",
    "\n",
    "So:\n",
    "- Long sentences → tiny probabilities  \n",
    "- Short sentences → bigger probabilities  \n",
    "\n",
    "This makes comparisons **unfair**.\n",
    "\n",
    "---\n",
    "\n",
    "To remove the effect of sentence length, we **average** the probabilities using the **geometric mean**.\n",
    "\n",
    "For a sentence with `n` words:\n",
    "\n",
    "$$\n",
    "P_{\\text{norm}}(W) = P(W)^{1/n}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "P_{\\text{norm}}(\\text{\"a red fox.\"}) = 0.0469^{1/4} = 0.465\n",
    "$$\n",
    "\n",
    "This means that on average, the model assigns about **46.5% confidence per word**.\n",
    "\n",
    "---\n",
    "\n",
    "Perplexity is just the **inverse** of this normalized probability:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(W) = \\frac{1}{P_{\\text{norm}}(W)}\n",
    "$$\n",
    "\n",
    "or  \n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(W) = \\left(\\frac{1}{P(W)}\\right)^{1/n}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\text{PP} = \\frac{1}{0.465} \\approx 2.15\n",
    "$$\n",
    "\n",
    "### Interpretation:\n",
    "- Perplexity ≈ 2 => the model feels like it's choosing between **2 reasonable words** at each step\n",
    "\n",
    "So we can see that a bad model has high perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22aa1c7",
   "metadata": {},
   "source": [
    "### How we are calculating the Perplexity in code\n",
    "\n",
    "Perplexity calculation happens in 4 main steps:\n",
    "1. Tokenization: The text is converted into token-id's which GPT2 understands.\n",
    "2. Teacher Forcing: The key is labels=input_ids. This basically tells GPT2:\n",
    "- here is your input sequence\n",
    "- now predict each subsequent token\n",
    "- calculate loss\n",
    "\n",
    "3. Cross-Entropy Loss: GPT2 calculates its loss (how wrong each prediction was).\n",
    "- For each prediction, it predicts a probability distribution of all possible next tokens.\n",
    "- The loss is measured as the gap between predicted probability and the actual token.\n",
    "\n",
    "4. Next, we want to convert loss to probability. `perplexity = exp(loss)`\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE:** Perplexity is also affected by temperature. As temperature increases, the AI becomes more non-deterministic, and so perplexity increases significantly as well. We use a temperature of 1.0 for creation of class 2 and class 3. This will result in higher than expected, however, it will still mostly be lower than that of humans. Generally, AI achieves human levels of perplexity only at temperatures >1.5. [Source - Peeperkorn et al., 2024](https://arxiv.org/html/2405.00492v1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset directory: /dataset\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Adjust this path to where you stored the dataset in Google Drive\n",
    "    DATASET_DIR = Path('/content/drive/MyDrive/precog-my-dataset/dataset')\n",
    "except ImportError:\n",
    "    DATASET_DIR = Path('../dataset')\n",
    "    print(f\"Running locally\")\n",
    "\n",
    "print(f\"Using dataset directory: {DATASET_DIR.resolve()}\")\n",
    "\n",
    "# Verify the path exists\n",
    "if DATASET_DIR.exists():\n",
    "    print(f\"Dataset directory found!\")\n",
    "else:\n",
    "    print(f\"Dataset directory NOT found at {DATASET_DIR.resolve()}\")\n",
    "    print(f\"Please upload your dataset folder to Google Drive and adjust the path above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e79fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model and tokenizer...\n",
      "(This may take a bit if downloading for the first time)\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a574aff67946abb5619a69e270667d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c222837d696b45d996d39a06222b147e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840638cfc1bc411b93cc14bcbc7fcc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bb2c056893412c87993bbcfa6df9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7098bfb600d41739d4e06e84b84b039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416a30e7bd89407c9273c19535195db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62826bbb3e24965bf87fb72d0b8c3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee05976db104314ae26d57f7d254a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Using 1 worker processes for file reading\n",
      "\n",
      "Processing Class 1 (Human)...\n",
      "Found 0 text files\n",
      "Reading files in parallel using 1 workers...\n",
      "Read 0 valid text files\n",
      "\n",
      "Processing Class 2 (AI)...\n",
      "Found 0 text files\n",
      "Reading files in parallel using 1 workers...\n",
      "Read 0 valid text files\n",
      "\n",
      "Processing Class 3 (AI Mimicry)...\n",
      "Found 0 text files\n",
      "Reading files in parallel using 1 workers...\n",
      "Read 0 valid text files\n",
      "\n",
      "======================================================================\n",
      "PERPLEXITY SUMMARY\n",
      "======================================================================\n",
      "Class                     Mean            Median          Files     \n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import numpy as np\n",
    "import statistics\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Loading GPT-2 model and tokenizer...\")\n",
    "print(\"(This may take a bit if downloading for the first time)\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Load model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "    gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    # Set padding token (GPT-2 doesn't have one by default)\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"\\nTip: If you're getting timeout errors, try running:\")\n",
    "    print(\"  export HF_HUB_DOWNLOAD_TIMEOUT=300\")\n",
    "    print(\"in your terminal before running this cell, or download manually.\")\n",
    "\n",
    "def calculate_perplexity_batch(texts, batch_size=8):\n",
    "    \"\"\"Calculate perplexity for a batch of texts\"\"\"\n",
    "    perplexities = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            # Tokenize batch\n",
    "            encodings = gpt2_tokenizer(\n",
    "                batch, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=1024,\n",
    "                padding=True\n",
    "            )\n",
    "            input_ids = encodings.input_ids.to(device)\n",
    "            attention_mask = encodings.attention_mask.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Calculate per-sample loss\n",
    "                for j in range(len(batch)):\n",
    "                    sample_input_ids = input_ids[j:j+1]\n",
    "                    sample_attention_mask = attention_mask[j:j+1]\n",
    "                    \n",
    "                    # Only calculate loss on non-padded tokens\n",
    "                    # Create labels with -100 for padding tokens (ignored in loss)\n",
    "                    labels = sample_input_ids.clone()\n",
    "                    labels[sample_attention_mask == 0] = -100\n",
    "                    \n",
    "                    sample_outputs = model(\n",
    "                        sample_input_ids, \n",
    "                        attention_mask=sample_attention_mask, \n",
    "                        labels=labels\n",
    "                    )\n",
    "                    loss = sample_outputs.loss\n",
    "                    perplexity = torch.exp(loss).item()\n",
    "                    perplexities.append(perplexity)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch processing: {e}\")\n",
    "            # Fallback to individual processing for this batch\n",
    "            for text in batch:\n",
    "                try:\n",
    "                    encodings = gpt2_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "                    input_ids = encodings.input_ids.to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(input_ids, labels=input_ids)\n",
    "                        loss = outputs.loss\n",
    "                        perplexity = torch.exp(loss).item()\n",
    "                    perplexities.append(perplexity)\n",
    "                except:\n",
    "                    perplexities.append(None)\n",
    "    \n",
    "    return perplexities\n",
    "\n",
    "def read_file_safe(file_path):\n",
    "    \"\"\"Safely read a file and return its content\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text.strip() if text else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_text_files_from_paths(path_list):\n",
    "    \"\"\"Get all .txt files from a list of directory paths\"\"\"\n",
    "    txt_files = []\n",
    "    for path in path_list:\n",
    "        if path.exists():\n",
    "            for file in path.glob('*.txt'):\n",
    "                txt_files.append(file)\n",
    "        else:\n",
    "            print(f\"Warning: Path does not exist: {path}\")\n",
    "    return txt_files\n",
    "\n",
    "# Define paths matching the structure from task-1.ipynb\n",
    "# Class 1: Human-written\n",
    "class1_paths = [\n",
    "    DATASET_DIR / 'class1-human-written' / '01-arthur-conan-doyle' / 'extracted_paragraphs',\n",
    "    DATASET_DIR / 'class1-human-written' / '02-pg-wodehouse' / 'extracted_paragraphs',\n",
    "    DATASET_DIR / 'class1-human-written' / '03-mark-twain' / 'extracted_paragraphs',\n",
    "    DATASET_DIR / 'class1-human-written' / '04-william-shakespeare' / 'extracted_paragraphs'\n",
    "]\n",
    "\n",
    "# Class 2: AI-written\n",
    "class2_paths = [\n",
    "    DATASET_DIR / 'class2-ai-written' / 'ai-generated-paragraphs'\n",
    "]\n",
    "\n",
    "# Class 3: AI-mimicry\n",
    "class3_paths = [\n",
    "    DATASET_DIR / 'class3-ai-mimicry' / '01-arthur-conan-doyle',\n",
    "    DATASET_DIR / 'class3-ai-mimicry' / '02-pg-wodehouse',\n",
    "    DATASET_DIR / 'class3-ai-mimicry' / '03-mark-twain',\n",
    "    DATASET_DIR / 'class3-ai-mimicry' / '04-william-shakespeare'\n",
    "]\n",
    "\n",
    "perplexity_results = {}\n",
    "\n",
    "for class_name, class_paths in [(\"Class 1 (Human)\", class1_paths), \n",
    "                                 (\"Class 2 (AI)\", class2_paths), \n",
    "                                 (\"Class 3 (AI Mimicry)\", class3_paths)]:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing {class_name}...\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Get all text files\n",
    "    text_files = get_text_files_from_paths(class_paths)\n",
    "    print(f\"Found {len(text_files)} text files\")\n",
    "    \n",
    "    if len(text_files) == 0:\n",
    "        print(f\"No files found. Please check the paths:\")\n",
    "        for path in class_paths:\n",
    "            print(f\"  - {path} (exists: {path.exists()})\")\n",
    "        continue\n",
    "    \n",
    "    # Read all files (without multiprocessing - doesn't work well in notebooks)\n",
    "    print(f\"Reading files...\")\n",
    "    all_texts = []\n",
    "    for file_path in tqdm(text_files, desc=\"Reading files\"):\n",
    "        text = read_file_safe(file_path)\n",
    "        if text:\n",
    "            all_texts.append(text)\n",
    "    \n",
    "    print(f\"Read {len(all_texts)} valid text files\")\n",
    "    \n",
    "    if all_texts:\n",
    "        print(f\"Calculating perplexities in batches of 8...\")\n",
    "        # Process in batches of 8\n",
    "        perplexities = calculate_perplexity_batch(all_texts, batch_size=8)\n",
    "        \n",
    "        # Filter out None values\n",
    "        perplexities = [p for p in perplexities if p is not None]\n",
    "        \n",
    "        if perplexities:\n",
    "            mean_perplexity = statistics.mean(perplexities)\n",
    "            median_perplexity = statistics.median(perplexities)\n",
    "            \n",
    "            perplexity_results[class_name] = {\n",
    "                'mean': mean_perplexity,\n",
    "                'median': median_perplexity,\n",
    "                'count': len(perplexities)\n",
    "            }\n",
    "            \n",
    "            print(f\"Mean Perplexity: {mean_perplexity:.2f}\")\n",
    "            print(f\"Median Perplexity: {median_perplexity:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERPLEXITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Class':<25} {'Mean':<15} {'Median':<15} {'Files':<10}\")\n",
    "print(\"-\"*70)\n",
    "for class_name, stats in perplexity_results.items():\n",
    "    print(f\"{class_name:<25} {stats['mean']:<15.2f} {stats['median']:<15.2f} {stats['count']:<10}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
